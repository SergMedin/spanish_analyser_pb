#!/usr/bin/env python3
"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–∞—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ —Å–∏—Å—Ç–µ–º–µ –∞–Ω–∞–ª–∏–∑–∞ (CLI).
–ü–µ—Ä–µ–º–µ—â—ë–Ω –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞ –≤ `tools/text_analyzer/` –¥–ª—è –ø–æ—Ä—è–¥–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.
"""

import sys
from pathlib import Path

# –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –ø–∞–∫–µ—Ç—É –∏–∑ src –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∑–∞–ø—É—Å–∫–∞
PROJECT_ROOT = Path(__file__).resolve().parents[2]
SRC_PATH = PROJECT_ROOT / "src"
if str(SRC_PATH) not in sys.path:
    sys.path.insert(0, str(SRC_PATH))

from spanish_analyser.config import config  # noqa: E402
from spanish_analyser.components.anki_connector import AnkiConnector  # noqa: E402
import re  # noqa: E402


class WordInvestigator:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤"""

    def __init__(self):
        self.config = config
        self.anki = None
        self.downloads_path = Path(self.config.get_downloads_folder())

    def connect_to_anki(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ AnkiConnect"""
        print("üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ AnkiConnect...")
        self.anki = AnkiConnector()
        if self.anki.is_available():
            print("‚úÖ AnkiConnect –¥–æ—Å—Ç—É–ø–µ–Ω")
            return True
        else:
            print("‚ùå AnkiConnect –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False

    def search_word_in_html_files(self, word: str):
        """–ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å–ª–æ–≤–∞ –≤ HTML —Ñ–∞–π–ª–∞—Ö (–±–µ–∑ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏)"""
        print(f"\nüîç –ü–æ–∏—Å–∫ —Å–ª–æ–≤–∞ '{word}' –≤ HTML —Ñ–∞–π–ª–∞—Ö:")

        found_files = []
        total_occurrences = 0

        html_files = list(self.downloads_path.glob("*.html"))
        total_files = len(html_files)
        print(f"üìÅ –ü—Ä–æ–≤–µ—Ä—è—é {total_files} HTML —Ñ–∞–π–ª–æ–≤...")

        target = word.lower()

        for i, html_file in enumerate(html_files):
            try:
                if total_files and (i % 50 == 0 or i == total_files - 1):
                    progress = (i + 1) / total_files * 100
                    print(f"   üîç –ü–æ–∏—Å–∫: {i + 1}/{total_files} ({progress:.1f}%)")
                with open(html_file, 'r', encoding='utf-8') as f:
                    content = f.read().lower()

                count = content.count(target)
                if count > 0:
                    found_files.append((html_file.name, count))
                    total_occurrences += count
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {html_file.name}: {e}")

        if found_files:
            print(f"‚úÖ –°–ª–æ–≤–æ '{word}' –Ω–∞–π–¥–µ–Ω–æ –≤ {len(found_files)} —Ñ–∞–π–ª–∞—Ö")
            print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ö–æ–∂–¥–µ–Ω–∏–π: {total_occurrences}")
            found_files.sort(key=lambda x: x[1], reverse=True)
            print(f"\nüìÇ –¢–æ–ø —Ñ–∞–π–ª–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤—Ö–æ–∂–¥–µ–Ω–∏–π:")
            for i, (filename, count) in enumerate(found_files[:5]):
                print(f"   {i+1}. {filename}: {count} —Ä–∞–∑")
            self._show_word_context(found_files[0][0], word)
        else:
            print(f"‚ùå –°–ª–æ–≤–æ '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ HTML —Ñ–∞–π–ª–∞—Ö")

        return total_occurrences, len(found_files)

    def _show_word_context(self, filename: str, word: str):
        """–ü–æ–∫–∞–∑–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ñ–∞–π–ª–∞"""
        print(f"\nüìù –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞ {filename}:")
        filepath = self.downloads_path / filename
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            match = pattern.search(content)
            if match:
                start = max(0, match.start() - 100)
                end = min(len(content), match.end() + 100)
                context = content[start:end]
                from bs4 import BeautifulSoup

                soup = BeautifulSoup(context, 'html.parser')
                clean_context = soup.get_text(separator=" ", strip=True)
                print(f"   ...{clean_context}...")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")

    def check_word_in_anki(self, word: str):
        """–ü—Ä—è–º–æ–π –±—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å–ª–æ–≤–∞ –≤ Anki"""
        print(f"\nüîç –ü–æ–∏—Å–∫ —Å–ª–æ–≤–∞ '{word}' –≤ Anki:")
        if not self.anki:
            print("‚ùå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Anki –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
            return []
        try:
            decks = self.anki.find_spanish_decks("Spanish")
            found = []
            for deck in decks:
                card_ids = self.anki.get_cards_from_deck(deck)
                cards_info = self.anki.get_cards_info(card_ids)
                note_ids = [c.get('note') for c in cards_info if c.get('note')]
                notes_info = self.anki.get_notes_info(note_ids)
                target = word.lower()
                for note in notes_info:
                    fields = note.get('fields', {})
                    for field_name, field_data in fields.items():
                        val = (field_data or {}).get('value', '')
                        if val and target in val.lower():
                            found.append((note.get('noteId'), field_name, val[:120] + ('...' if len(val) > 120 else '')))
                            if len(found) >= 3:
                                break
                    if len(found) >= 3:
                        break
                if len(found) >= 3:
                    break
            if found:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(found)} –∫–∞—Ä—Ç–æ—á–µ–∫ —Å–æ —Å–ª–æ–≤–æ–º '{word}':")
                for i, (nid, fname, preview) in enumerate(found, 1):
                    print(f"   {i}. noteId={nid} field={fname}: {preview}")
            else:
                print(f"‚ùå –ö–∞—Ä—Ç–æ—á–∫–∏ —Å–æ —Å–ª–æ–≤–æ–º '{word}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return found
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ Anki: {e}")
            return []

    def investigate_word(self, word: str):
        """–ë—ã—Å—Ç—Ä–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Å–ª–æ–≤–∞ (production-—Ä–µ–∂–∏–º)"""
        print("=" * 60)
        print(f"üîç –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï –°–õ–û–í–ê: '{word}'")
        print("=" * 60)

        # 1) –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–∞—Ö
        html_occurrences, html_files = self.search_word_in_html_files(word)

        # 2) –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ –≤ Anki
        anki_cards = self.check_word_in_anki(word)

        # 3) –†–µ—à–µ–Ω–∏–µ: –ø–æ–ø–∞–¥—ë—Ç –ª–∏ –≤ –≤—ã–≥—Ä—É–∑–∫—É?
        # –í Excel –ø–æ–ø–∞–¥–∞—é—Ç –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ (–∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ Anki).
        will_be_exported = html_occurrences > 0 and len(anki_cards) == 0
        status = "–î–∞" if will_be_exported else "–ù–µ—Ç"

        print("\nüìä –ò–¢–û–ì:")
        print(f"   üìÑ –í —Ç–µ–∫—Å—Ç–∞—Ö: {'–Ω–∞–π–¥–µ–Ω–æ' if html_occurrences > 0 else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'} (–≤—Ö–æ–∂–¥–µ–Ω–∏–π: {html_occurrences})")
        print(f"   üìö –í Anki: {'–Ω–∞–π–¥–µ–Ω–æ' if anki_cards else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}")
        print(f"   üìÅ –ü–æ–ø–∞–¥—ë—Ç –≤ Excel –≤—ã–≥—Ä—É–∑–∫—É: {status}")
        print("=" * 60)

    def run_cli(self, word: str):
        if not self.connect_to_anki():
            return 1
        self.investigate_word(word)
        if self.anki:
            self.anki.disconnect()
            print("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Anki –∑–∞–∫—Ä—ã—Ç–æ")
        return 0


def main():
    import argparse

    parser = argparse.ArgumentParser(description='–ë—ã—Å—Ç—Ä—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤')
    parser.add_argument('word', help='–°–ª–æ–≤–æ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, "c√∫bico")')
    args = parser.parse_args()

    investigator = WordInvestigator()
    raise SystemExit(investigator.run_cli(args.word))


if __name__ == '__main__':
    main()





