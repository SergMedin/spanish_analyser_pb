#!/usr/bin/env python3
"""
–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –ø–æ—à–∞–≥–æ–≤—ã–π –¥–µ–º–æ-—Å–∫—Ä–∏–ø—Ç: –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å ANKI.

–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç:
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞ (`config.yaml`)
- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ –∏—Å–ø–∞–Ω—Å–∫–æ–º
- –ü–æ—à–∞–≥–æ–≤–æ –≤—ã–ø–æ–ª–Ω—è–µ—Ç: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é ‚Üí POS-—Ç–µ–≥–∏ ‚Üí –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é ‚Üí —á–∞—Å—Ç–æ—Ç—ã ‚Üí —Å–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
- –ù–û–í–û–ï: –ü–æ–¥—Ä–æ–±–Ω–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å ANKI:
  * –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ AnkiConnect
  * –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Notes (–∑–∞–º–µ—Ç–æ–∫) –∏–∑ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥
  * –õ–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
  * –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
- –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤—ã–≤–æ–¥–∏—Ç –ø–æ–Ω—è—Ç–Ω—ã–µ –ø–æ—è—Å–Ω–µ–Ω–∏—è –∏ —Ç–∞–±–ª–∏—Ü—ã
- –ñ–¥—ë—Ç –Ω–∞–∂–∞—Ç–∏—è –ø—Ä–æ–±–µ–ª–∞, —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —à–∞–≥—É

–ö–∞–∫ –∑–∞–ø—É—Å–∫–∞—Ç—å:
1) –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å venv: `source venv/bin/activate`
2) –£–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ ANKI –∑–∞–ø—É—â–µ–Ω —Å –ø–ª–∞–≥–∏–Ω–æ–º AnkiConnect (–∫–æ–¥: 2055492159)
3) –ó–∞–ø—É—Å—Ç–∏—Ç—å: `python tools/dev_scripts/word_analyzer_simple_demo.py`

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: —Å–∫—Ä–∏–ø—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–∑ `src/spanish_analyser/...`.
–ï—Å–ª–∏ ANKI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å–∫—Ä–∏–ø—Ç –ø–æ–∫–∞–∂–µ—Ç –¥–µ–º–æ-—Ä–µ–∂–∏–º —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.
"""

import os
import sys
import time
from typing import List

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞)
sys.path.append('src')

from spanish_analyser.config import config
from spanish_analyser.components.tokenizer import TokenProcessor
from spanish_analyser.components.pos_tagger import POSTagger
from spanish_analyser.components.lemmatizer import LemmaProcessor
from spanish_analyser.components.frequency_analyzer import FrequencyAnalyzer
from spanish_analyser.components.text_pipeline import SpanishTextPipeline
from spanish_analyser.components.word_comparator import WordComparator
from spanish_analyser.interfaces.text_processor import WordInfo, AnalysisResult
from spanish_analyser.word_analyzer import WordAnalyzer


TEST_TEXT = (
    "La casa es muy grande y hermosa. Yo corro r√°pido en el parque todos los d√≠as. "
    "Este libro es muy interesante para estudiar. El ni√±o come frutas frescas. "
    "Adem√°s, la capital de Espa√±a es Madrid, una ciudad moderna. Barcelona tambi√©n es importante. "
    "El capital inicial fue insuficiente para el proyecto. Mar√≠a trabaj√≥ en Par√≠s. "
    "Las capitales europeas son fascinantes, pero los capitales extranjeros dominan el mercado. "
    "El herido est√° estable en el hospital, mientras que la herida necesita limpieza urgente. "
    "Las heridas cicatrizan lentamente, pero los heridos se recuperan bien. "
    "Ella es muy bonita y elegante, √©l es bonito y simp√°tico. "
    "Tambi√©n vemos formas como bonita, bonito, bonitas y bonitos en descripciones literarias. "
    "La mesa est√° limpia. El agua est√° fr√≠a. Los libros est√°n ordenados. "
    "Las personas trabajadoras merecen respeto. Los trabajadores industriales son importantes. "
    "La econom√≠a mundial depende de m√∫ltiples factores complejos y variables. Google es una empresa americana."
)


def pause(step_title: str) -> None:
    print()
    print(f"–ù–∞–∂–º–∏—Ç–µ –ü–†–û–ë–ï–õ, —á—Ç–æ–±—ã –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å: {step_title}")
    try:
        # –ß–∏—Ç–∞–µ–º –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ –±–µ–∑ Enter, –Ω–æ –∫—Ä–æ—Å—Å–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ–Ω–Ω–æ –ø—Ä–æ—â–µ –∂–¥–∞—Ç—å Enter.
        # –î–ª—è UX ‚Äî –ø—Ä–∏–Ω–∏–º–∞–µ–º –∏ –ø—Ä–æ–±–µ–ª, –∏ Enter.
        user_input = input()
        # –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤—ë–ª –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–µ–ª ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º.
    except KeyboardInterrupt:
        print("\n–û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
        sys.exit(0)


def pretty_rule(title: str) -> None:
    print("\n" + title)
    print("=" * len(title))


def show_table(headers: List[str], rows: List[List[str]], max_rows: int = 20) -> None:
    if not rows:
        print("(–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)")
        return
    
    total_rows = len(rows)
    is_truncated = total_rows > max_rows
    displayed_rows = rows[:max_rows]
    
    widths = [len(h) for h in headers]
    for r in displayed_rows:
        for i, cell in enumerate(r):
            widths[i] = max(widths[i], len(str(cell)))
    def fmt_row(values: List[str]) -> str:
        return " | ".join(str(v).ljust(widths[i]) for i, v in enumerate(values))
    sep = "-+-".join("-" * w for w in widths)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—É—Å–∞ —Ç–∞–±–ª–∏—Ü—ã –ü–ï–†–ï–î —Ç–∞–±–ª–∏—Ü–µ–π
    if is_truncated:
        print(f"üìã –¢–∞–±–ª–∏—Ü–∞ (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {max_rows} –∏–∑ {total_rows} —Å—Ç—Ä–æ–∫):")
    else:
        print(f"üìã –¢–∞–±–ª–∏—Ü–∞ (–≤—Å–µ {total_rows} —Å—Ç—Ä–æ–∫):")
    
    print(fmt_row(headers))
    print(sep)
    for r in displayed_rows:
        print(fmt_row(r))
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –ü–û–°–õ–ï —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
    if is_truncated:
        print(f"... (–ø–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {max_rows} –∏–∑ {total_rows} —Å—Ç—Ä–æ–∫)")


def main() -> None:
    pretty_rule("–®–∞–≥ 0. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞")
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω (–õ–£–ß–®–ê–Ø –ü–†–ê–ö–¢–ò–ö–ê)
    pipeline = SpanishTextPipeline(min_word_length=config.get_min_word_length())
    
    print(f"–ü—É—Ç—å –∫ config.yaml: {config.config_path}")
    print(f"spaCy –º–æ–¥–µ–ª—å: {config.get_spacy_model()}")
    print(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞: {config.get_min_word_length()}")
    print(f"–ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {config.get_results_folder()}")
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É")

    pretty_rule("–®–∞–≥ 1. –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç")
    print(TEST_TEXT)
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É")

    pretty_rule("–®–∞–≥ 2. –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)")
    print("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º —á–µ—Ä–µ–∑ spaCy...")
    context = pipeline.analyze_text(TEST_TEXT)
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
    valid_tokens = pipeline.get_filtered_tokens(context)
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(context.tokens)} —Ç–æ–∫–µ–Ω–æ–≤, {len(valid_tokens)} –≤–∞–ª–∏–¥–Ω—ã—Ö")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {context.processing_time_ms:.1f} –º—Å")
    print(f"üìñ –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(context.sentences)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è
    print("\nüìù –ü—Ä–∏–º–µ—Ä—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∞–Ω–∞–ª–∏–∑–µ):")
    valid_examples = valid_tokens[:5]
    for token in valid_examples:
        print(f"   ‚úì '{token.text}' (POS: {token.pos}, –ª–µ–º–º–∞: {token.lemma})")
    
    print("\nüö´ –ü—Ä–∏–º–µ—Ä—ã –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è):")
    filtered_out = [t for t in context.tokens if not t.is_valid][:5]
    for token in filtered_out:
        reason = []
        if not token.is_alpha:
            reason.append("–Ω–µ –±—É–∫–≤—ã")
        punct_chars = ".,!?;:"
        if token.text.strip() in punct_chars:
            reason.append("–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è")
        if not token.text.strip():
            reason.append("–ø—Ä–æ–±–µ–ª")
        if len(token.text) < pipeline.min_word_length:
            reason.append(f"< {pipeline.min_word_length} —Å–∏–º–≤–æ–ª–æ–≤")
        reason_str = ", ".join(reason) if reason else "–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω"
        print(f"   ‚úó '{token.text}' ({reason_str})")
    
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–±–æ—Ä—É")

    pretty_rule("–®–∞–≥ 3. –î–µ—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∑–±–æ—Ä: POS, –†–æ–¥, –õ–µ–º–º–∞")
    pos_tagger = POSTagger(model_name=config.get_spacy_model())  # –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ POS –Ω–∞ —Ä—É—Å—Å–∫–∏–π
    
    rows = []
    for i, token in enumerate(valid_tokens, start=1):
        gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        rows.append([
            str(i), 
            token.text, 
            token.pos, 
            pos_tagger.get_pos_tag_ru(token.pos), 
            gender or "-",
            token.lemma
        ])
    
    show_table(["#", "–¢–æ–∫–µ–Ω", "POS", "POS (RU)", "Gender", "–õ–µ–º–º–∞"], rows)
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏ —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏")

    pretty_rule("–®–∞–≥ 4. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏ (–∫–∞–∫ –±—É–¥–µ—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ)")
    print("üîç –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ –ª–µ–º–º—ã —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É—é—Ç—Å—è –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ Excel:")
    
    lemma_rows = []
    for i, token in enumerate(valid_tokens, start=1):
        gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        if token.pos == 'NOUN':
            excel_format = pipeline.format_noun_with_article(token.lemma, gender)
        else:
            excel_format = token.lemma
        
        lemma_rows.append([
            str(i), 
            token.text, 
            token.lemma,
            excel_format,
            gender or "-"
        ])
    
    show_table(["#", "–¢–æ–∫–µ–Ω", "–õ–µ–º–º–∞", "–§–æ—Ä–º–∞—Ç –¥–ª—è Excel", "–†–æ–¥"], lemma_rows)
    print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≤ –∫–æ–ª–æ–Ω–∫–µ '–§–æ—Ä–º–∞—Ç –¥–ª—è Excel' —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞–Ω—ã —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏")
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ —á–∞—Å—Ç–æ—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É")

    pretty_rule("–®–∞–≥ 5. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (NOUN —Å –∞—Ä—Ç–∏–∫–ª–µ–º –ø–æ —Ä–æ–¥—É, –æ—Å—Ç–∞–ª—å–Ω—ã–µ ‚Äî –ø–æ –ª–µ–º–º–µ)")
    freq = FrequencyAnalyzer()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    freq_tokens = []
    for token in valid_tokens:
        if token.pos == 'NOUN':
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–µ—Ç–æ–¥ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∞—Ä—Ç–∏–∫–ª–µ–º
            gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
            key = pipeline.format_noun_with_article(token.lemma, gender)
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–º–º—É
            key = token.lemma
        freq_tokens.append(key)
    
    freq_map = freq.count_frequency(freq_tokens)
    most_common = freq.get_most_frequent(20)
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª—é—á–µ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏: {len(freq_map)}")
    show_table(["–ö–ª—é—á —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏", "–ß–∞—Å—Ç–æ—Ç–∞"], [[w, str(c)] for w, c in most_common])
    
    # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
    print("\nüìñ –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–ø-—Å–ª–æ–≤:")
    for freq_key, count in most_common[:5]:
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω —Å —ç—Ç–∏–º –∫–ª—é—á–æ–º
        for token in valid_tokens:
            token_key = pipeline.format_noun_with_article(token.lemma, 
                token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None) if token.pos == 'NOUN' else token.lemma
            if token_key == freq_key:
                ctx = pipeline.get_context_around_token(context, token, window=3)
                print(f"  {freq_key}: ...{ctx}...")
                break
    
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ —Å–±–æ—Ä–∫–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")

    pretty_rule("–®–∞–≥ 6. –°–±–æ—Ä–∫–∞ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (–∫–∞–∫ –≤ Excel —ç–∫—Å–ø–æ—Ä—Ç–µ)")
    print("üìã –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö:")
    
    # –°–æ–∑–¥–∞—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ
    final_rows = []
    for i, token in enumerate(valid_tokens, start=1):
        gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        
        # –§–æ—Ä–º–∞—Ç –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–ª–æ–≤–∞ (–∫–∞–∫ –≤ Excel)
        if token.pos == 'NOUN':
            word_display = pipeline.format_noun_with_article(token.lemma, gender)
            freq_key = word_display
        else:
            word_display = token.text
            freq_key = token.lemma
        
        frequency = freq_map.get(freq_key, 0)
        
        final_rows.append([
            str(i),
            word_display,  # –°–ª–æ–≤–æ —Å –∞—Ä—Ç–∏–∫–ª–µ–º –¥–ª—è NOUN
            token.lemma,
            pos_tagger.get_pos_tag_ru(token.pos),
            gender or "-",
            str(frequency),
            "–ù–µ—Ç"  # is_known (–¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –≤—Å–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ)
        ])
    
    show_table([
        "#", "–°–ª–æ–≤–æ (–¥–ª—è Excel)", "–õ–µ–º–º–∞", "–ß–∞—Å—Ç—å —Ä–µ—á–∏", "–†–æ–¥", "–ß–∞—Å—Ç–æ—Ç–∞", "–ò–∑–≤–µ—Å—Ç–Ω–æ"
    ], final_rows, max_rows=15)
    
    print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≤ –∫–æ–ª–æ–Ω–∫–µ '–°–ª–æ–≤–æ (–¥–ª—è Excel)' —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞–Ω—ã —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏")
    print("üìä –≠—Ç–æ –∏–º–µ–Ω–Ω–æ —Ç–æ, —á—Ç–æ –ø–æ–ø–∞–¥—ë—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π Excel —Ñ–∞–π–ª")
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç—ã —Å ANKI")

    pretty_rule("–®–∞–≥ 7. –ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ª–æ–≥–∏–∫–∏ —Ä–∞–±–æ—Ç—ã —Å ANKI")
    print("üîó –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å ANKI: –æ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –¥–æ –ø–æ–∏—Å–∫–∞ —Å–ª–æ–≤")
    
    print("\nüìã 7.1. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ANKI —á–µ—Ä–µ–∑ AnkiConnect")
    print("   ‚ö° –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å AnkiConnect API...")
    
    # –°–æ–∑–¥–∞—ë–º word_comparator –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    demo_comparator = WordComparator()
    
    if demo_comparator.anki_connector.is_available():
        print("   ‚úÖ AnkiConnect –¥–æ—Å—Ç—É–ø–µ–Ω!")
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏
        conn_info = demo_comparator.anki_connector.get_connection_info()
        print(f"   üìä –í–µ—Ä—Å–∏—è AnkiConnect: {conn_info.get('version', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞')}")
        print(f"   üìö –í—Å–µ–≥–æ –∫–æ–ª–æ–¥ –≤ ANKI: {conn_info.get('total_decks', 0)}")
        print(f"   üá™üá∏ –ò—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥: {len(conn_info.get('spanish_decks', []))}")
        
        pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é Notes –∏–∑ ANKI")
        
        print("\nüìã 7.2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ Notes (–∑–∞–º–µ—Ç–æ–∫) –∏–∑ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥")
        print("   üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø—Ä–æ—Å: deck:Spanish*")
        print("   üìù –í–ê–ñ–ù–û: –†–∞–±–æ—Ç–∞–µ–º —Å Notes –Ω–∞–ø—Ä—è–º—É—é, –∞ –Ω–µ —á–µ—Ä–µ–∑ Cards!")
        print("   üí° –ö–∞–∂–¥–æ–µ –ø–æ–ª–µ FrontText —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ –æ–¥–∏–Ω —Ç–µ—Ä–º–∏–Ω –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        spanish_terms = demo_comparator.known_words
        
        print(f"   ‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(spanish_terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ ANKI")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ç–µ—Ä–º–∏–Ω–æ–≤
        print("\n   üìù –ü—Ä–∏–º–µ—Ä—ã —Ç–µ—Ä–º–∏–Ω–æ–≤ (–ø–µ—Ä–≤—ã–µ 8):")
        sample_terms = sorted(list(spanish_terms))[:8]
        for i, term in enumerate(sample_terms, 1):
            print(f"      {i:2d}. \"{term}\"")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∏–ø–æ–≤ —Ç–µ—Ä–º–∏–Ω–æ–≤
        phrases = [t for t in spanish_terms if ' ' in t]
        single_words = [t for t in spanish_terms if ' ' not in t]
        print(f"\n   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤:")
        print(f"      üî§ –û—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(single_words)}")
        print(f"      üìñ –§—Ä–∞–∑: {len(phrases)} (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'comprar un billete', 'abrir la puerta')")
        
        pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ª–æ–≥–∏–∫–∏ –ø–æ–∏—Å–∫–∞")
        
        print("\nüìã 7.3. –õ–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤ (–°–¢–†–û–ì–û)")
        print("   üéØ –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç—Å—è –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å:")
        print("   1Ô∏è‚É£ –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç–µ—Ä–º–∏–Ω–æ–º –≤ ANKI")
        print("   üö´ –í—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –≤–æ —Ñ—Ä–∞–∑—É –ù–ï —Å—á–∏—Ç–∞–µ—Ç—Å—è –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å—é")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        demo_words = [
            ("conductor", "–æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ"),
            ("el conductor", "—Å–ª–æ–≤–æ —Å –∞—Ä—Ç–∏–∫–ª–µ–º"),
            ("comprar", "—Å–ª–æ–≤–æ –∏–∑ —Ñ—Ä–∞–∑—ã"),
            ("billete", "—Å–ª–æ–≤–æ –∏–∑ —Ñ—Ä–∞–∑—ã"),
            ("especial", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ"),
        ]
        
        demo_rows = []
        for word, description in demo_words:
            is_known = demo_comparator.is_word_known(word)
            
            # –ò—â–µ–º –≤ –∫–∞–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–∞—Ö ANKI –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è —ç—Ç–æ —Å–ª–æ–≤–æ
            found_examples = []
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if word.lower() in spanish_terms:
                found_examples.append(f"[—Ç–æ—á–Ω–æ–µ] '{word.lower()}'")
            
            # –î–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ –º–æ–∂–µ–º —É–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Å–ª–æ–≤–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑
            import re
            pattern = r'\\b' + re.escape(word.lower()) + r'\\b'
            phrase_hits = [term for term in sorted(spanish_terms) if ' ' in term and re.search(pattern, term)]
            if phrase_hits and word.lower() not in spanish_terms:
                found_examples.append("(–≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑ ‚Äî –Ω–µ —Å—á–∏—Ç–∞–µ—Ç—Å—è)")
            
            found_text = "; ".join(found_examples) if found_examples else "‚Äî"
            status = "‚úÖ –ò–∑–≤–µ—Å—Ç–Ω–æ" if is_known else "‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            
            demo_rows.append([word, description, status, found_text[:60] + "..." if len(found_text) > 60 else found_text])
        
        show_table([
            "–°–ª–æ–≤–æ", "–¢–∏–ø", "–°—Ç–∞—Ç—É—Å", "–ù–∞–π–¥–µ–Ω–æ –≤ ANKI"
        ], demo_rows)
        
        pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤")
        
        print("\nüìã 7.4. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤")
        print("   üîç –ê–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö:")
        print("   1Ô∏è‚É£ –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)")
        print("   2Ô∏è‚É£ –°–ª–æ–≤–∞, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ç–æ–π –∂–µ –æ—Å–Ω–æ–≤—ã") 
        print("   3Ô∏è‚É£ –î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–Ω—ã—Ö –≥–ª–∞–≥–æ–ª–æ–≤ (—Å 'se') ‚Äî –ø–æ–∏—Å–∫ –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã")
        
        # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö
        similarity_examples = [
            ("especial", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ"),
            ("ubicar", "–≤–æ–∑–º–æ–∂–Ω–æ –µ—Å—Ç—å –ø–æ—Ö–æ–∂–∏–µ"),
            ("fantasma", "–ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤–æ–µ —Å–ª–æ–≤–æ"),
        ]
        
        similar_rows = []
        for word, description in similarity_examples:
            is_known = demo_comparator.is_word_known(word)
            similar = demo_comparator.get_similar_candidates(
                lemma=word, pos='NOUN', gender=None
            )
            
            status = "‚úÖ –ò–∑–≤–µ—Å—Ç–Ω–æ" if is_known else "‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
            similar_text = ", ".join(similar[:3]) if similar else "–ù–µ—Ç –ø–æ—Ö–æ–∂–∏—Ö"
            if len(similar) > 3:
                similar_text += f" (+ –µ—â—ë {len(similar) - 3})"
            
            similar_rows.append([word, description, status, similar_text])
        
        show_table([
            "–°–ª–æ–≤–æ", "–û–ø–∏—Å–∞–Ω–∏–µ", "–°—Ç–∞—Ç—É—Å", "–ü–æ—Ö–æ–∂–∏–µ –≤ ANKI"
        ], similar_rows)
        
        print("\nüí° –≠—Ç–∏ '–ø–æ—Ö–æ–∂–∏–µ' —Å–ª–æ–≤–∞ –ø–æ–ø–∞–¥—É—Ç –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ Excel, –Ω–æ –ù–ï –¥–µ–ª–∞—é—Ç —Å–ª–æ–≤–æ '–∏–∑–≤–µ—Å—Ç–Ω—ã–º'")
        
        pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
        
    else:
        print("   ‚ö†Ô∏è AnkiConnect –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ")
        
        pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –ø—Ä–æ–≤–µ—Ä–∫–µ —Å–ª–æ–≤ (–¥–µ–º–æ-—Ä–µ–∂–∏–º)")

    pretty_rule("–®–∞–≥ 8. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ ANKI")
    print("üîç –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–£ –ñ–ï –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞...")
    
    # –°–æ–∑–¥–∞—ë–º WordAnalyzer –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º ANKI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é (–∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–¥–µ)
    analyzer = WordAnalyzer()
    anki_success = analyzer.init_anki_integration()
    
    if anki_success:
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ ANKI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —á—Ç–æ –∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –≤ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer.add_words_from_text(TEST_TEXT)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
        all_words = list(analyzer.word_frequencies.most_common())[:10]
        
        anki_check_rows = []
        for i, (word_with_pos, freq) in enumerate(all_words, start=1):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–æ –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "—Å–ª–æ–≤–æ (—á–∞—Å—Ç—å_—Ä–µ—á–∏)"
            if ' (' in word_with_pos and word_with_pos.endswith(')'):
                word_part = word_with_pos.split(' (')[0]  # "el capital" –∏–ª–∏ "capital"
                pos_tag = word_with_pos.split(' (')[1].rstrip(')')
            else:
                word_part = word_with_pos
                pos_tag = '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–£ –ñ–ï –ª–æ–≥–∏–∫—É —á—Ç–æ –≤ export_to_excel –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
            is_known = analyzer.word_comparator.is_word_known(word_part)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¢–ê–ö –ñ–ï –∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –∫–æ–¥–µ
            comment = ""
            if not is_known:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –ª–µ–º–º—É –¥–ª—è get_similar_candidates
                if word_part.startswith(('el ', 'la ')):
                    base_lemma = word_part.split(' ', 1)[1]
                else:
                    base_lemma = word_part
                
                token_info = analyzer.token_details.get(base_lemma, {})
                similar = analyzer.word_comparator.get_similar_candidates(
                    lemma=base_lemma,
                    pos=token_info.get('pos', 'UNKNOWN'),
                    gender=token_info.get('gender')
                )
                if similar:
                    comment = "–ü–æ—Ö–æ–∂–∏–µ –≤ ANKI: " + ", ".join(similar)
                else:
                    comment = "–ù–æ–≤–æ–µ —Å–ª–æ–≤–æ"
            
            anki_check_rows.append([
                str(i),
                word_part,
                "–î–∞" if is_known else "–ù–µ—Ç",
                comment or "‚Äî"
            ])
    else:
        print("‚ö†Ô∏è ANKI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ")
        anki_check_rows = [
            ["1", "la casa", "–ù–µ—Ç", "–î–µ–º–æ-—Ä–µ–∂–∏–º"],
            ["2", "muy", "–ù–µ—Ç", "–î–µ–º–æ-—Ä–µ–∂–∏–º"],
            ["3", "grande", "–ù–µ—Ç", "–î–µ–º–æ-—Ä–µ–∂–∏–º"]
        ]
    
    show_table([
        "#", "–°–ª–æ–≤–æ", "–ò–∑–≤–µ—Å—Ç–Ω–æ –≤ ANKI", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è Excel"
    ], anki_check_rows)
    
    print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –∫–æ–ª–æ–Ω–∫–∞ '–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è Excel' –ø–æ–ø–∞–¥—ë—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç")
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã —Å 'capital'")

    pretty_rule("–®–∞–≥ 9. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã –æ–º–æ–Ω–∏–º–æ–≤ (capital)")
    print("üéØ –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞–µ—Ç—Å—è 'la capital' –∏ 'el capital':")
    
    capital_tokens = [t for t in valid_tokens if 'capital' in t.text]
    for i, token in enumerate(capital_tokens, 1):
        gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        formatted = pipeline.format_noun_with_article(token.lemma, gender)
        ctx = pipeline.get_context_around_token(context, token, window=4)
        
        print(f"\n{i}. –¢–æ–∫–µ–Ω: '{token.text}'")
        print(f"   –õ–µ–º–º–∞: {token.lemma}")
        print(f"   POS: {token.pos} ({pos_tagger.get_pos_tag_ru(token.pos)})")
        print(f"   –†–æ–¥: {gender or '–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω'}")
        print(f"   –ö–ª—é—á —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏: '{formatted}'")
        print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç: ...{ctx}...")
    
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö")

    pretty_rule("–®–∞–≥ 10. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö (PROPN ‚Üí NOUN)")
    print("üèõÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫—É –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏:")
    print("üí° –ü—Ä–∞–≤–∏–ª–æ –ø—Ä–æ–µ–∫—Ç–∞: PROPN –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–µ–º–∞–ø—è—Ç—Å—è –≤ NOUN –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è")
    
    # –ù–∞–π–¥–µ–º –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤ —Ç–µ–∫—Å—Ç–µ
    propn_tokens = [t for t in valid_tokens if t.pos == 'PROPN']
    
    if propn_tokens:
        print(f"\nüìç –ù–∞–π–¥–µ–Ω–æ {len(propn_tokens)} –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≤ —Ç–µ–∫—Å—Ç–µ:")
        propn_rows = []
        for i, token in enumerate(propn_tokens, 1):
            gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –±—ã–ª–æ –∏ —á—Ç–æ —Å—Ç–∞–ª–æ –ø–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
            original_pos = "PROPN"
            corrected_pos = "NOUN"  # –ü–æ –ø—Ä–∞–≤–∏–ª–∞–º –ø—Ä–æ–µ–∫—Ç–∞
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∫ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ
            formatted = pipeline.format_noun_with_article(token.lemma, gender)
            
            propn_rows.append([
                str(i),
                token.text,
                original_pos,
                corrected_pos,
                pos_tagger.get_pos_tag_ru(corrected_pos),
                gender or "–ù–µ–∏–∑–≤–µ—Å—Ç–µ–Ω",
                formatted
            ])
        
        show_table([
            "#", "–ò–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ", "–ò—Å—Ö–æ–¥–Ω—ã–π POS", "–ü–æ—Å–ª–µ –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏", "POS (RU)", "–†–æ–¥", "–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç"
        ], propn_rows)
        
        print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ò–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ")
        print("üîÑ –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤–∫–ª—é—á–∏—Ç—å –∏—Ö –≤ –æ–±—â—É—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é –ø–æ –ª–µ–º–º–∞–º")
    else:
        print("\n‚ùå –í –¥–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö (PROPN)")
        print("üí° –û–±—ã—á–Ω–æ –∑–¥–µ—Å—å –±—ã–ª–∏ –±—ã: Madrid ‚Üí el madrid, Espa√±a ‚Üí la espa√±a")
    
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–æ–¥–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö")

    pretty_rule("–®–∞–≥ 11. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–æ–¥–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    print("‚öñÔ∏è –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è —Ä–æ–¥, –µ—Å–ª–∏ spaCy –µ–≥–æ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª:")
    print("üîç –ê–ª–≥–æ—Ä–∏—Ç–º: –∏—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å (DET) —Å–ª–µ–≤–∞ –æ—Ç —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ")
    
    # –°–æ–∑–¥–∞–¥–∏–º –ø—Ä–∏–º–µ—Ä—ã —Å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º —Ä–æ–¥–æ–º
    noun_tokens = [t for t in valid_tokens if t.pos == 'NOUN']
    
    gender_examples = []
    for token in noun_tokens:
        original_gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–æ–¥–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∫–∞–∫ –¥–µ–ª–∞–µ—Ç pipeline)
        recovered_gender = original_gender
        recovery_method = "–ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω spaCy"
        
        if not original_gender:
            # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å —Å–ª–µ–≤–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –¥–µ–º–æ)
            token_position = valid_tokens.index(token)
            for j in range(max(0, token_position - 3), token_position):
                prev_token = valid_tokens[j]
                if prev_token.pos == 'DET':
                    if prev_token.text.lower() in ['el', 'un', 'este', 'ese', 'aquel']:
                        recovered_gender = 'Masc'
                        recovery_method = f"–ò–∑ DET '{prev_token.text}'"
                        break
                    elif prev_token.text.lower() in ['la', 'una', 'esta', 'esa', 'aquella']:
                        recovered_gender = 'Fem'
                        recovery_method = f"–ò–∑ DET '{prev_token.text}'"
                        break
            
            if not recovered_gender:
                recovery_method = "–ù–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å"
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º —Ä–æ–¥–æ–º
        if recovered_gender:
            article = "el" if recovered_gender == 'Masc' else "la"
            formatted = f"{article} {token.lemma}"
        else:
            formatted = token.lemma
        
        gender_examples.append([
            token.text,
            token.lemma,
            original_gender or "‚Äî",
            recovered_gender or "‚Äî",
            recovery_method,
            formatted
        ])
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 8 –ø—Ä–∏–º–µ—Ä–æ–≤
    show_table([
        "–¢–æ–∫–µ–Ω", "–õ–µ–º–º–∞", "–†–æ–¥ (spaCy)", "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π", "–ú–µ—Ç–æ–¥", "–ò—Ç–æ–≥–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç"
    ], gender_examples[:8])
    
    print("\nüí° –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –µ—Å–ª–∏ —Ä–æ–¥ –Ω–µ —É–¥–∞–µ—Ç—Å—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å, —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∞—Ä—Ç–∏–∫–ª—è")
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏")

    pretty_rule("–®–∞–≥ 12. –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è")
    print("üîÑ –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –¥–ª—è Excel:")
    print("üìã –®–∞–≥–∏ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏:")
    print("   1Ô∏è‚É£ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ª–µ–º–º–µ + POS + —Ä–æ–¥")
    print("   2Ô∏è‚É£ –°—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Å—Ç–æ—Ç –≤ –≥—Ä—É–ø–ø–∞—Ö")
    print("   3Ô∏è‚É£ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Å–ø–∏—Å–∫–∞ WordInfo")
    print("   4Ô∏è‚É£ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–æ–ª—é 'Word' —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ Count")
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–µ
    from collections import defaultdict
    
    # –®–∞–≥ 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
    print(f"\nüîç –®–∞–≥ 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ {len(valid_tokens)} —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ –ª–µ–º–º–µ+POS+—Ä–æ–¥")
    groups = defaultdict(list)
    
    for token in valid_tokens:
        gender = token.morph.get('Gender', [None])[0] if 'Gender' in token.morph else None
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ü–∏—é PROPN ‚Üí NOUN
        corrected_pos = 'NOUN' if token.pos == 'PROPN' else token.pos
        
        key = (token.lemma, corrected_pos, gender)
        groups[key].append(token)
    
    print(f"   üìä –ü–æ–ª—É—á–µ–Ω–æ {len(groups)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥—Ä—É–ø–ø")
    
    # –®–∞–≥ 2: –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç
    print(f"\nüîç –®–∞–≥ 2: –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã")
    group_stats = []
    
    for (lemma, pos, gender), tokens in list(groups.items())[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10
        count = len(tokens)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ —Å–ª–æ–≤–æ
        if pos == 'NOUN':
            display_word = pipeline.format_noun_with_article(lemma, gender)
        else:
            display_word = lemma
        
        group_stats.append([
            display_word,
            lemma,
            pos_tagger.get_pos_tag_ru(pos),
            gender or "‚Äî",
            str(count),
            f"{len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤"
        ])
    
    show_table([
        "–û—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ —Å–ª–æ–≤–æ", "–õ–µ–º–º–∞", "POS (RU)", "–†–æ–¥", "–ß–∞—Å—Ç–æ—Ç–∞", "–ò—Å—Ç–æ—á–Ω–∏–∫"
    ], group_stats)
    
    pause("–ø–µ—Ä–µ–π—Ç–∏ –∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏")
    
    # –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ WordInfo –æ–±—ä–µ–∫—Ç–æ–≤
    print(f"\nüîç –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ WordInfo (–∫–∞–∫ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞)")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å –¥—É–±–ª—è–º–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    demo_words = [
        # –ü—Ä–∏–º–µ—Ä –¥—É–±–ª–µ–π –¥–ª—è —Å–ª–æ–≤–∞ "medio"
        WordInfo(word="medio", lemma="medio", pos_tag="NUM", pos_tag_ru="–ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ", 
                frequency=23, is_known=False, gender=None),
        WordInfo(word="el medio", lemma="medio", pos_tag="NOUN", pos_tag_ru="–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ", 
                frequency=17, is_known=False, gender="Masc"),
        WordInfo(word="medio", lemma="medio", pos_tag="ADJ", pos_tag_ru="–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ", 
                frequency=19, is_known=False, gender=None),
        WordInfo(word="medio", lemma="medio", pos_tag="ADV", pos_tag_ru="–ù–∞—Ä–µ—á–∏–µ", 
                frequency=1, is_known=False, gender=None),
        # –ï—â–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        WordInfo(word="la casa", lemma="casa", pos_tag="NOUN", pos_tag_ru="–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ", 
                frequency=5, is_known=False, gender="Fem"),
        WordInfo(word="grande", lemma="grande", pos_tag="ADJ", pos_tag_ru="–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ", 
                frequency=3, is_known=False, gender=None),
    ]
    
    print(f"üìù –ò—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫ (—Å –¥—É–±–ª—è–º–∏): {len(demo_words)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Å–ø–∏—Å–æ–∫
    before_dedup = []
    for i, word_info in enumerate(demo_words, 1):
        before_dedup.append([
            str(i),
            word_info.word,
            word_info.pos_tag_ru,
            word_info.gender or "‚Äî",
            str(word_info.frequency),
            "–î–∞" if word_info.is_known else "–ù–µ—Ç"
        ])
    
    show_table([
        "#", "Word", "POS", "–†–æ–¥", "Count", "–ò–∑–≤–µ—Å—Ç–Ω–æ"
    ], before_dedup)
    
    print("\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ï—Å—Ç—å –¥—É–±–ª–∏ –ø–æ –ø–æ–ª—é 'Word' (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'medio' –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è 3 —Ä–∞–∑–∞)")
    pause("–ø—Ä–∏–º–µ–Ω–∏—Ç—å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é")
    
    # –®–∞–≥ 4: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
    print(f"\nüîç –®–∞–≥ 4: –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–æ–ª—é 'Word' (–Ω–∞—à–∞ –Ω–µ–¥–∞–≤–Ω—è—è –∏—Å–ø—Ä–∞–≤–∫–∞)")
    print("   üìã –ê–ª–≥–æ—Ä–∏—Ç–º:")
    print("   1Ô∏è‚É£ –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Word (asc), –∑–∞—Ç–µ–º –ø–æ Count (desc)")
    print("   2Ô∏è‚É£ –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Count")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
    import pandas as pd
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame –∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ
    excel_data = []
    for word_info in demo_words:
        excel_data.append({
            'Word': word_info.word,
            'Lemma': word_info.lemma,
            'Part of Speech': word_info.pos_tag_ru,
            'Gender': word_info.gender or '-',
            'Count': word_info.frequency,
            'Comments': '–ù–æ–≤–æ–µ —Å–ª–æ–≤–æ'
        })
    
    df = pd.DataFrame(excel_data)
    
    print(f"\nüìä –î–û –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(df)} —Å—Ç—Ä–æ–∫")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—É –∂–µ –ª–æ–≥–∏–∫—É —á—Ç–æ –≤ word_analyzer.py
    before_count = len(df)
    df = df.sort_values(['Word', 'Count'], ascending=[True, False], kind='stable')
    df = df.drop_duplicates(subset=['Word'], keep='first').reset_index(drop=True)
    after_count = len(df)
    
    print(f"üìä –ü–û–°–õ–ï –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(df)} —Å—Ç—Ä–æ–∫ (—É–¥–∞–ª–µ–Ω–æ: {before_count - after_count})")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    after_dedup = []
    for i, (_, row) in enumerate(df.iterrows(), 1):
        after_dedup.append([
            str(i),
            row['Word'],
            row['Part of Speech'],
            row['Gender'],
            str(row['Count']),
            row['Comments']
        ])
    
    show_table([
        "#", "Word", "POS", "–†–æ–¥", "Count", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏"
    ], after_dedup)
    
    print("\n‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢: –î–ª—è 'medio' –æ—Å—Ç–∞–ª–∞—Å—å —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Count (23)")
    print("‚úÖ –ö–∞–∂–¥–æ–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ Word –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑")
    print("‚úÖ –≠—Ç–æ –∏–º–µ–Ω–Ω–æ —Ç–æ, —á—Ç–æ –ø–æ–ø–∞–¥–µ—Ç –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π Excel —Ñ–∞–π–ª")
    
    pause("–∑–∞–≤–µ—Ä—à–∏—Ç—å –¥–µ–º–æ")

    pretty_rule("üéâ –ò–¢–û–ì–ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–ò")
    print("–ú—ã –ø–æ–¥—Ä–æ–±–Ω–æ –∏–∑—É—á–∏–ª–∏ –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤:")
    print()
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ spaCy —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
    print("‚úÖ –ö–æ—Ä—Ä–µ–∫—Ü–∏—è POS: PROPN ‚Üí NOUN –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    print("‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–æ–¥–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª–µ–π")
    print("‚úÖ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–∏–∫–ª–µ–π: el/la + –ª–µ–º–º–∞ –¥–ª—è NOUN")
    print("‚úÖ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–æ –≥—Ä—É–ø–ø–∞–º (–ª–µ–º–º–∞ + POS + —Ä–æ–¥)")
    print("‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ANKI –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏")
    print("‚úÖ –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –ø–æ–ª—é Word —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º max Count")
    print()
    print("üîÑ –ü—Ä–æ—Ü–µ—Å—Å –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–∞–≤–∏–ª–∞–º –ø—Ä–æ–µ–∫—Ç–∞:")
    print("   üìñ –û–¥–∏–Ω –∞–ª–≥–æ—Ä–∏—Ç–º ‚Äî –æ–¥–Ω–æ –º–µ—Å—Ç–æ –≤ –∫–æ–¥–µ")
    print("   üéØ –ù–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ–º–æ–Ω–∏–º–æ–≤ (la capital ‚â† el capital)")
    print("   üîß –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö")


if __name__ == "__main__":
    main()


