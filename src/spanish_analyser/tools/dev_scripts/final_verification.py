#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
"""

import sys
import spacy
import yaml
import os
import tempfile
import pandas as pd

def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ config.yaml"""
    config_path = "config.yaml"
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f) or {}
    return {}

def test_spacy_integration():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é spaCy –≤ WordAnalyzer"""
    print("üß™ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π")
    print("=" * 50)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config_data = load_config()
    spacy_model = config_data.get('text_analysis', {}).get('spacy_model', 'es_core_news_md')

    print(f"–ú–æ–¥–µ–ª—å spaCy –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {spacy_model}")

    # –£–î–ê–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–¥–∏–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ POSTagger
    from spanish_analyser.components.pos_tagger import POSTagger
    pos_tagger = POSTagger()

    # –ë–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ (–∫–æ–ø–∏—è –∏–∑ word_analyzer.py)
    def determine_pos_basic(word: str) -> str:
        """–ë–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏"""
        word_lower = word.lower()

        # –ü—Ä–æ—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        if word_lower in ['el', 'la', 'los', 'las', 'un', 'una', 'unos', 'unas']:
            return "–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å"
        elif word_lower in ['y', 'o', 'pero', 'si', 'que', 'como', 'cuando', 'donde']:
            return "—Å–æ—é–∑"
        elif word_lower in ['yo', 't√∫', '√©l', 'ella', 'nosotros', 'nosotras', 'vosotros', 'vosotras', 'ellos', 'ellas']:
            return "–º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ"
        elif word_lower in ['a', 'ante', 'bajo', 'cabe', 'con', 'contra', 'de', 'desde', 'durante', 'en', 'entre', 'hacia', 'hasta', 'mediante', 'para', 'por', 'seg√∫n', 'sin', 'so', 'sobre', 'tras']:
            return "–ø—Ä–µ–¥–ª–æ–≥"
        elif word_lower.endswith(('ar', 'er', 'ir')):
            return "–≥–ª–∞–≥–æ–ª"
        elif word_lower.endswith(('ado', 'ido', 'ada', 'ida')):
            return "–ø—Ä–∏—á–∞—Å—Ç–∏–µ"
        elif word_lower.endswith(('ando', 'iendo', 'endo')):
            return "–≥–µ—Ä—É–Ω–¥–∏–π"
        elif word_lower.endswith(('oso', 'osa', 'al', 'ar', 'ivo', 'iva', 'able', 'ible')):
            return "–ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ"
        elif word_lower.endswith(('mente')):
            return "–Ω–∞—Ä–µ—á–∏–µ"
        elif word_lower.endswith(('ci√≥n', 'si√≥n', 'dad', 'tad', 'tud', 'ez', 'eza', 'ura', '√≠a', 'io')):
            return "—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ"
        elif word_lower.isdigit() or word_lower in ['primero', 'segundo', 'tercero', 'cuarto', 'quinto']:
            return "—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ"

        return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ —Å spaCy
    def determine_pos_with_spacy(word: str, nlp) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é spaCy"""
        if not nlp:
            return determine_pos_basic(word)

        try:
            doc = nlp(word)
            if doc:
                token = doc[0]
                pos_tag = token.pos_
                return pos_tagger.get_pos_tag_ru(pos_tag)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–ª–æ–≤–∞ '{word}' —Å spaCy: {e}")

        return "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    # –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–∏ —Ä–µ—á–∏
    def determine_pos(word: str, nlp) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–∏ —Ä–µ—á–∏"""
        if nlp:
            return determine_pos_with_spacy(word, nlp)
        else:
            print(f"‚ö†Ô∏è spaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å–ª–æ–≤–∞ '{word}', –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
            return determine_pos_basic(word)

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É spaCy
    nlp = None
    try:
        nlp = spacy.load(spacy_model)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å spaCy {spacy_model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ spaCy: {e}")
        print("‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ spaCy, –∏—Å–ø–æ–ª—å–∑—É—è –±–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
    test_words = [
        "casa",      # —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (–¥–æ–º)
        "correr",    # –≥–ª–∞–≥–æ–ª (–±–µ–≥–∞—Ç—å)
        "r√°pido",    # –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (–±—ã—Å—Ç—Ä—ã–π)
        "en",        # –ø—Ä–µ–¥–ª–æ–≥ (–≤)
        "yo",        # –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ (—è)
        "feliz",     # –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (—Å—á–∞—Å—Ç–ª–∏–≤—ã–π)
        "comer",     # –≥–ª–∞–≥–æ–ª (–µ—Å—Ç—å)
        "mesa",      # —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ (—Å—Ç–æ–ª)
        "con",       # –ø—Ä–µ–¥–ª–æ–≥ (—Å)
        "muy"        # –Ω–∞—Ä–µ—á–∏–µ (–æ—á–µ–Ω—å)
    ]

    print()
    print("üìù –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏:")
    print("-" * 40)

    results = []
    for word in test_words:
        pos = determine_pos(word, nlp)
        results.append((word, pos))
        print("12")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ—Ç "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ" –µ—Å–ª–∏ spaCy —Ä–∞–±–æ—Ç–∞–µ—Ç
    unknown_count = sum(1 for word, pos in results if pos == "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ")

    print()
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print("-" * 40)

    if nlp:
        print(f"‚úÖ spaCy —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        print(f"üìã –û–ø—Ä–µ–¥–µ–ª–µ–Ω–æ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏: {len(test_words) - unknown_count} –∏–∑ {len(test_words)}")
        if unknown_count == 0:
            print("üéâ –£—Å–ø–µ—Ö! –í—Å–µ —Å–ª–æ–≤–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print(f"‚ö†Ô∏è {unknown_count} —Å–ª–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∫–∞–∫ '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'")
    else:
        print("‚ö†Ô∏è spaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ")
        print(f"üìã –ë–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {len(test_words) - unknown_count} –∏–∑ {len(test_words)}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–º–∏—Ç–∞—Ü–∏—é Excel —ç–∫—Å–ø–æ—Ä—Ç–∞
    print()
    print("üìà –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ Excel:")
    print("-" * 40)

    excel_data = []
    for word, pos in results:
        excel_data.append({
            'Word': word,
            'Part of Speech': pos,
            'Frequency': '1.00%',  # –∏–º–∏—Ç–∞—Ü–∏—è
            'Count': 1
        })

    df = pd.DataFrame(excel_data)
    print("üìã –î–∞–Ω–Ω—ã–µ –¥–ª—è Excel:")
    for i, row in df.iterrows():
        print(f"  {row['Word']}: {row['Part of Speech']}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤ Part of Speech –Ω–µ—Ç "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ" –µ—Å–ª–∏ spaCy —Ä–∞–±–æ—Ç–∞–µ—Ç
    if nlp and unknown_count == 0:
        print()
        print("üéâ –ò–¢–û–ì: –ü—Ä–æ–±–ª–µ–º–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –†–ï–®–ï–ù–ê!")
        print("‚úÖ –í Excel —Ñ–∞–π–ª–µ —Ç–µ–ø–µ—Ä—å –±—É–¥—É—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏")
    elif not nlp:
        print()
        print("‚ö†Ô∏è –ò–¢–û–ì: spaCy –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –±–∞–∑–æ–≤–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
        print("üí° –î–ª—è –ø–æ–ª–Ω–æ–π —Ä–∞–±–æ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å: python -m spacy download es_core_news_md")
    else:
        print()
        print(f"‚ö†Ô∏è –ò–¢–û–ì: {unknown_count} —Å–ª–æ–≤ –≤—Å–µ –µ—â–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã")
        print("üîç –ù—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏–∫—É –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏")

if __name__ == "__main__":
    test_spacy_integration()


