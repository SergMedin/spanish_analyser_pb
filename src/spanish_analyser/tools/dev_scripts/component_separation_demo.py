"""
–î–µ–º–æ-—Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ WordAnalyzer —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç—É –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ –∏—Ö –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é.
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))

from spanish_analyser.word_analyzer import WordAnalyzer


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ."""
    print("üîç –î–µ–º–æ –Ω–æ–≤–æ–≥–æ WordAnalyzer —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏")
    print("=" * 50)
    
    try:
        # –°–æ–∑–¥–∞—ë–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        print("üìù –°–æ–∑–¥–∞—é WordAnalyzer...")
        analyzer = WordAnalyzer(
            min_word_length=3,
            spacy_model="es_core_news_md",
            output_dir="data/results"
        )
        
        print("‚úÖ WordAnalyzer —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç
        test_text = """
        Hola mundo espa√±ol. El ni√±o est√° aqu√≠ con la ni√±a.
        Tengo 5 manzanas y 3 peras. ¬øC√≥mo est√°s?
        """
        
        print(f"\nüìñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç:")
        print(f"'{test_text.strip()}'")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        print("\nüîç –ù–∞—á–∏–Ω–∞—é –∞–Ω–∞–ª–∏–∑...")
        result = analyzer.analyze_text(test_text)
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
        print(f"  –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤: {result.total_words}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {result.unique_words}")
        print(f"  –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: {len(result.unknown_words)}")
        print(f"  –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {result.processing_time:.2f} —Å–µ–∫")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è
        print(f"\nüìö –°–ª–æ–≤–∞ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è:")
        unknown_words = analyzer.get_unknown_words_for_learning(result)
        for i, word_info in enumerate(unknown_words[:10], 1):
            print(f"  {i:2d}. {word_info.word:<15} ({word_info.pos_tag_ru:<12}) –ß–∞—Å—Ç–æ—Ç–∞: {word_info.frequency}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:")
        stats = analyzer.get_statistics()
        print(f"  –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {stats['tokenizer']['valid_tokens']} –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤")
        print(f"  –õ–µ–º–º–∞—Ç–∏–∑–∞—Ç–æ—Ä: –∫—ç—à {stats['lemmatizer']['cache_size']} —Å–ª–æ–≤")
        print(f"  POS-—Ç–µ–≥–≥–µ—Ä: –º–æ–¥–µ–ª—å {stats['pos_tagger']['model_name']} –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {stats['pos_tagger']['model_loaded']}")
        print(f"  –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å: {stats['frequency_analyzer']['total_words']} —Å–ª–æ–≤")
        print(f"  –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {stats['word_comparator']['known_words_count']} –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤")
        print(f"  –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä: –∫—ç—à {stats['word_normalizer']['cache_size']} —Å–ª–æ–≤")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–∫—Å–ø–æ—Ä—Ç
        print(f"\nüíæ –¢–µ—Å—Ç–∏—Ä—É—é —ç–∫—Å–ø–æ—Ä—Ç...")
        exported_files = analyzer.export_results(result, "demo_analysis")
        
        print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(exported_files)} —Ñ–∞–π–ª–æ–≤:")
        for format_name, file_path in exported_files.items():
            print(f"  {format_name}: {file_path}")
        
        print(f"\nüéâ –î–µ–º–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –¥–µ–º–æ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
