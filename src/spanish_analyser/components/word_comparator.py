"""
–ö–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–ª–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏.

–û—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –∑–∞–≥—Ä—É–∑–∫—É –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ Anki, –ø—Ä–æ–≤–µ—Ä–∫—É –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏
–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è.
"""

import os
import sqlite3
from typing import List, Dict, Set, Union, Optional, Tuple
from pathlib import Path
from ..interfaces.text_processor import WordComparatorInterface
from ..config import config
from ..cache import CacheManager  # –ú–µ–Ω–µ–¥–∂–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–æ–¥–ø–∞–ø–æ–∫
from .normalizer import WordNormalizer
from .anki_connector import AnkiConnector
import time
import re

try:
    import spacy
except Exception:
    spacy = None  # type: ignore


import logging


logger = logging.getLogger(__name__)


class WordComparator(WordComparatorInterface):
    """–ö–æ–º–ø–∞—Ä–∞—Ç–æ—Ä –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–ª–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏."""
    
    def __init__(self, collection_path: Optional[str] = None, deck_pattern: str = "Spanish", text_model: Optional["BaseTextModel"] = None, autoload: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä —Å–ª–æ–≤.
        
        Args:
            collection_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö Anki (—É—Å—Ç–∞—Ä–µ–ª, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è AnkiConnect)
            deck_pattern: –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥
        """
        self.collection_path = collection_path or self._get_default_collection_path()
        self.deck_pattern = deck_pattern
        self.known_words: Set[str] = set()  # —Ç–æ—á–Ω—ã–µ —Å–ª–æ–≤–∞ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
        self.normalized_known_words: Set[str] = set()  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        self.known_phrases: Set[str] = set()  # —Ç–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø–æ–ª–µ–π (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
        # –ò–Ω–¥–µ–∫—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ spaCy (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω):
        # NOUN —Å —Ä–æ–¥–æ–º: (lemma_lower, gender) ‚Üí –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ ANKI
        self.known_noun_lemma_gender: Set[Tuple[str, str]] = set()
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ POS: (lemma_lower, pos) ‚Üí –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ ANKI
        self.known_lemma_pos: Set[Tuple[str, str]] = set()
        self.normalizer = WordNormalizer(use_cache=True)
        self._nlp = None
        self.anki_connector = AnkiConnector()
        self._text_model = text_model
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
        if autoload:
            self._load_known_words_modern()
    
    def _load_known_words_modern(self) -> None:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π AnkiConnect API.
        """
        try:
            if not self.anki_connector.is_available():
                logger.warning("üîó AnkiConnect –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
                logger.info("üí° –î–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Anki:")
                logger.info("   1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Anki")
                logger.info("   2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–ª–∞–≥–∏–Ω AnkiConnect (–∫–æ–¥: 2055492159)")
                logger.info("   3. –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ Anki")
                return
            
            logger.info("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ Anki...")
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥
            logger.debug(f"üîç –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å–ª–æ–≤–∞ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–æ–º –∫–æ–ª–æ–¥—ã: '{self.deck_pattern}'")
            spanish_words = self.anki_connector.extract_all_spanish_words(self.deck_pattern)
            
            if spanish_words:
                logger.debug(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: –ø–æ–ª—É—á–µ–Ω–æ {len(spanish_words)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ù–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é‚Ä¶")
                _t0 = time.time()
                self._load_from_list(list(spanish_words))
                logger.debug(f"üîß –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (dt={time.time()-_t0:.2f}s)")
                logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.known_words)} —Å–ª–æ–≤ –∏–∑ Anki —á–µ—Ä–µ–∑ AnkiConnect")
                
            else:
                logger.warning(f"–°–ª–æ–≤–∞ –∏–∑ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–ø–∞—Ç—Ç–µ—Ä–Ω: {self.deck_pattern})")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ AnkiConnect: {e}")
    
    def _get_default_collection_path(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ –±–∞–∑–µ Anki –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é."""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –û–°
        home = os.path.expanduser("~")
        
        if os.name == 'nt':  # Windows
            return os.path.join(home, "AppData", "Roaming", "Anki2", "User 1", "collection.anki2")
        elif os.name == 'posix':  # macOS –∏ Linux
            if os.path.exists(os.path.join(home, "Library")):  # macOS
                return os.path.join(home, "Library", "Application Support", "Anki2", "User 1", "collection.anki2")
            else:  # Linux
                return os.path.join(home, ".local", "share", "Anki2", "User 1", "collection.anki2")
        
        return ""
    
    def load_known_words(self, source: Union[str, Path, List[str]]) -> None:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞.
        
        Args:
            source: –ò—Å—Ç–æ—á–Ω–∏–∫ —Å–ª–æ–≤ (–ø—É—Ç—å –∫ Anki –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤)
        """
        if isinstance(source, list):
            self._load_from_list(source)
        else:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∫—ç—à
            collection_path_str = str(source)
            if config.should_cache_anki_words() and os.path.exists(collection_path_str):
                try:
                    mtime = os.path.getmtime(collection_path_str)
                    size = os.path.getsize(collection_path_str)
                    cache_key = f"anki_known_words:{collection_path_str}:{mtime}:{size}:{self.deck_pattern}"
                    cache = CacheManager.get_cache()
                    cached = cache.get(cache_key)
                    if cached is not None and isinstance(cached, dict):
                        self.known_words = set(cached.get("known_words", []))
                        self.normalized_known_words = set(cached.get("normalized_known_words", []))
                        return
                except Exception:
                    pass
            self._load_from_anki(collection_path_str)
            if config.should_cache_anki_words() and os.path.exists(collection_path_str):
                try:
                    cache.set(cache_key, {
                        "known_words": list(self.known_words),
                        "normalized_known_words": list(self.normalized_known_words),
                    })
                except Exception:
                    pass
    
    def _load_from_list(self, words: List[str]) -> None:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Å–ø–∏—Å–∫–∞.
        
        Args:
            words: –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        """
        logger.debug(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: —Å—Ç–∞—Ä—Ç (items_in={len(words)})")
        _t_start = time.time()
        self.known_words.clear()
        self.normalized_known_words.clear()
        self.known_phrases.clear()
        self.known_noun_lemma_gender.clear()
        self.known_lemma_pos.clear()
        
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º, —á—Ç–æ words —É–∂–µ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞; –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—ã
        _t_norm = time.time()
        for word in words:
            if word and isinstance(word, str):
                w = word.lower().strip()
                if not w:
                    continue
                self.known_words.add(w)
                normalized = self.normalizer.normalize(word)
                if normalized:
                    self.normalized_known_words.add(normalized)
        logger.debug(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (unique={len(self.known_words)}, dt={time.time()-_t_norm:.2f}s)")

        # –ü–æ—Å—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã –ª–µ–º–º+POS –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –æ—Ç–¥–µ–ª—å–Ω–æ, –∞ –Ω–µ –≤—Å–µ —Å—Ä–∞–∑—É
        try:
            if self._text_model is not None:
                _t_idx = time.time()
                logger.debug("üîé –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ text_model (–ø–æ –æ–¥–Ω–æ–º—É)‚Ä¶")
                for word in sorted(self.known_words):
                    if not word.strip():
                        continue
                    try:
                        result = self._text_model.analyze_text(word)
                        for tok in result.tokens:
                            lemma = (tok.lemma or "").lower()
                            pos = tok.pos or ""
                            if pos == 'NOUN':
                                # –ë–µ–∑ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏ ‚Äî –ø–æ–º–µ—á–∞–µ–º Unknown
                                self.known_noun_lemma_gender.add((lemma, 'Unknown'))
                            else:
                                self.known_lemma_pos.add((lemma, pos))
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ '{word}': {e}")
                logger.debug(f"üîé –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —á–µ—Ä–µ–∑ text_model –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (dt={time.time()-_t_idx:.2f}s)")
            elif spacy is not None:
                # –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ SpacyManager
                from .spacy_manager import SpacyManager
                if self._nlp is None:
                    self._nlp = SpacyManager().get_nlp()
                _t_spa = time.time()
                logger.debug(f"üîé –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ —á–µ—Ä–µ–∑ spaCy (–ø–æ –æ–¥–Ω–æ–º—É)‚Ä¶ (model={getattr(self._nlp, 'meta', {}).get('name', 'unknown')}, items={len(self.known_words)})")
                _noun_count = 0
                _processed_words = 0
                
                for word in sorted(self.known_words):
                    if not word.strip():
                        continue
                    try:
                        doc = self._nlp(word)
                        _processed_words += 1
                        
                        for token in doc:
                            if not token.text.strip():
                                continue
                            lemma = token.lemma_.lower()
                            pos = token.pos_
                            if pos == 'NOUN' or pos == 'PROPN':
                                # PROPN –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ NOUN –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º –ø—Ä–æ–µ–∫—Ç–∞)
                                gender_list = token.morph.get('Gender')
                                gender = gender_list[0] if gender_list else 'Unknown'
                                self.known_noun_lemma_gender.add((lemma, gender))
                                _noun_count += 1
                                
                            else:
                                self.known_lemma_pos.add((lemma, pos))
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ spaCy '{word}': {e}")
                logger.debug(f"üîé –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è spaCy –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (processed={_processed_words}, nouns={_noun_count})")
        except Exception:
            # –ù–µ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã
            pass
        finally:
            logger.debug(f"üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (total_dt={time.time()-_t_start:.2f}s)")
    
    def _load_from_anki(self, collection_path: str) -> None:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –±–∞–∑—ã Anki.
        
        Args:
            collection_path: –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö Anki
        """
        if not os.path.exists(collection_path):
            print(f"–ë–∞–∑–∞ Anki –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {collection_path}")
            return
        
        try:
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –∫–æ–ø–∏—é –±–∞–∑—ã –¥–ª—è —á—Ç–µ–Ω–∏—è
            temp_path = collection_path + "-temp"
            with open(collection_path, 'rb') as src, open(temp_path, 'wb') as dst:
                dst.write(src.read())
            
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–∞–∑–µ
            conn = sqlite3.connect(temp_path)
            cursor = conn.cursor()
            
            # –ò—â–µ–º –∏—Å–ø–∞–Ω—Å–∫–∏–µ –∫–æ–ª–æ–¥—ã
            cursor.execute("""
                SELECT name FROM col WHERE decks LIKE ?
            """, (f'%{self.deck_pattern}%',))
            
            deck_names = cursor.fetchall()
            if not deck_names:
                print(f"–ò—Å–ø–∞–Ω—Å–∫–∏–µ –∫–æ–ª–æ–¥—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {self.deck_pattern}")
                return
            
            # –ü–æ–ª—É—á–∞–µ–º ID –∫–æ–ª–æ–¥
            deck_ids = []
            for deck_name in deck_names:
                cursor.execute("""
                    SELECT decks FROM col
                """)
                decks_data = cursor.fetchone()
                if decks_data:
                    # –ü–∞—Ä—Å–∏–º JSON —Å –∫–æ–ª–æ–¥–∞–º–∏
                    import json
                    try:
                        decks = json.loads(decks_data[0])
                        for deck_id, deck_info in decks.items():
                            if self.deck_pattern.replace('*', '') in deck_info.get('name', ''):
                                deck_ids.append(deck_id)
                    except json.JSONDecodeError:
                        continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–¥
            known_words = set()
            for deck_id in deck_ids:
                cursor.execute("""
                    SELECT flds FROM notes n
                    JOIN cards c ON n.id = c.nid
                    WHERE c.did = ?
                """, (deck_id,))
                
                for (flds,) in cursor.fetchall():
                    if flds:
                        # –ü–∞—Ä—Å–∏–º –ø–æ–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏
                        fields = flds.split('\x1f')
                        for field in fields:
                            if field:
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—É—é —Ñ—Ä–∞–∑—É –∫–∞–∫ –µ—Å—Ç—å (—Å—Ç—Ä–æ–≥–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ)
                                field_norm = field.strip().lower()
                                if field_norm:
                                    self.known_phrases.add(field_norm)
                                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –ø–æ–ª—è
                                words = self._extract_words_from_field(field)
                                known_words.update(words)
            
            conn.close()
            os.remove(temp_path)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–∞
            self._load_from_list(list(known_words))
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.known_words)} –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ Anki")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤ –∏–∑ Anki: {e}")
            if os.path.exists(temp_path):
                os.remove(temp_path)
    
    def _extract_words_from_field(self, field: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ –ø–æ–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏ Anki.
        
        Args:
            field: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ–ª—è –∫–∞—Ä—Ç–æ—á–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö —Å–ª–æ–≤
        """
        if not field:
            return []
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)
        words = re.findall(r'\b[a-zA-Z√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú]+\b', field)
        return [word.lower() for word in words if len(word) >= 3]
    
    def is_word_known(self, word: str, phrase: Optional[str] = None) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–≤–µ—Å—Ç–Ω–æ –ª–∏ —Å–ª–æ–≤–æ.
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —Å–ª–æ–≤–æ –∏–∑–≤–µ—Å—Ç–Ω–æ
        """
        if not word:
            return False
        
        word_lower = word.lower().strip()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –¢–û–õ–¨–ö–û —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ (—Å—Ç—Ä–æ–≥–∞—è –ª–æ–≥–∏–∫–∞)
        # –ù–∞—Ö–æ–∂–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –≤–Ω—É—Ç—Ä–∏ —Ñ—Ä–∞–∑—ã –ù–ï –¥–µ–ª–∞–µ—Ç –µ–≥–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º
        if word_lower in self.known_words:
            return True
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ñ—Ä–∞–∑—ã (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞)
        if phrase:
            phrase_lower = phrase.lower().strip()
            if phrase_lower in self.known_phrases:
                return True
        
        # 3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: —Ä–µ–∂–∏–º –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ –ø–æ –ª–µ–º–º–µ/—á–∞—Å—Ç–∏ —Ä–µ—á–∏
        try:
            if config.is_lemma_aware_known_enabled():
                analysis_text = word_lower
                lemma = None
                pos = None
                if self._text_model is not None:
                    res = self._text_model.analyze_text(analysis_text)
                    if res.tokens:
                        lemma = (res.tokens[0].lemma or '').lower()
                        pos = res.tokens[0].pos or ''
                elif spacy is not None:
                    # –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ SpacyManager
                    from .spacy_manager import SpacyManager
                    if self._nlp is None:
                        self._nlp = SpacyManager().get_nlp()
                    doc = self._nlp(analysis_text)
                    if doc:
                        lemma = doc[0].lemma_.lower()
                        pos = doc[0].pos_
                if lemma and pos:
                    if pos == 'NOUN':
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –ª–µ–º–º–∞ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å –ª—é–±—ã–º —Ä–æ–¥–æ–º
                        if (lemma, 'Masc') in self.known_noun_lemma_gender or (lemma, 'Fem') in self.known_noun_lemma_gender or (lemma, 'Unknown') in self.known_noun_lemma_gender:
                            return True
                    else:
                        if (lemma, pos) in self.known_lemma_pos:
                            return True
        except Exception:
            pass

        return False

    def is_token_known(self, *, lemma: Optional[str], pos: Optional[str] = None, gender: Optional[str] = None) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ (lemma, pos, gender) –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ spaCy.

        Args:
            lemma: –õ–µ–º–º–∞ —Å–ª–æ–≤–∞ (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä)
            pos: POS-—Ç–µ–≥ spaCy (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'NOUN', 'VERB')
            gender: –†–æ–¥ –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö ('Masc'|'Fem'|'Unknown'|None)

        Returns:
            True –µ—Å–ª–∏ –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∏–∑–≤–µ—Å—Ç–Ω–∞ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º ANKI
        """
        try:
            if not lemma:
                logger.debug(f"üö´ is_token_known: –ø—É—Å—Ç–∞—è lemma")
                return False
            lemma_l = lemma.lower().strip()
            logger.debug(f"üîé is_token_known: lemma='{lemma_l}', pos={pos}, gender={gender}")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å—Ä–µ–¥–∏ —Ç–æ—á–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ (–∫–∞–∫ –µ—Å—Ç—å)
            if lemma_l in self.known_words:
                logger.debug(f"‚úÖ is_token_known: –Ω–∞–π–¥–µ–Ω–æ –≤ known_words: '{lemma_l}'")
                return True
                
            if not pos:
                logger.debug(f"üö´ is_token_known: –Ω–µ—Ç POS –¥–ª—è '{lemma_l}'")
                return False
                
            if pos == 'NOUN':
                logger.debug(f"üìù –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ: '{lemma_l}' —Å gender={gender}")
                
                # –õ—é–±–æ–π –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ä–æ–¥–æ–≤ –¥–ª—è –¥–∞–Ω–Ω–æ–π –ª–µ–º–º—ã –¥–µ–ª–∞–µ—Ç –µ—ë –∏–∑–≤–µ—Å—Ç–Ω–æ–π
                masc_known = (lemma_l, 'Masc') in self.known_noun_lemma_gender
                fem_known = (lemma_l, 'Fem') in self.known_noun_lemma_gender
                unk_known = (lemma_l, 'Unknown') in self.known_noun_lemma_gender
                
                logger.debug(f"   ‚ÑπÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ known_noun_lemma_gender: Masc={masc_known}, Fem={fem_known}, Unknown={unk_known}")
                
                if masc_known or fem_known or unk_known:
                    logger.debug(f"‚úÖ is_token_known: –Ω–∞–π–¥–µ–Ω–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ '{lemma_l}' –≤ known_noun_lemma_gender")
                    return True
                    
                # –ï—Å–ª–∏ gender —è–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–Ω ‚Äî –ø—Ä–æ–≤–µ—Ä–∏–º –∫–æ–Ω–∫—Ä–µ—Ç–∏–∫—É
                if gender in ('Masc', 'Fem', 'Unknown'):
                    specific_known = (lemma_l, gender) in self.known_noun_lemma_gender
                    logger.debug(f"   ‚ÑπÔ∏è –ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ({lemma_l}, {gender}): {specific_known}")
                    return specific_known
                    
                logger.debug(f"‚ùå is_token_known: —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ '{lemma_l}' –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return False
            else:
                # –ù–µ-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
                lemma_pos_known = (lemma_l, pos) in self.known_lemma_pos
                logger.debug(f"   ‚ÑπÔ∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ known_lemma_pos ({lemma_l}, {pos}): {lemma_pos_known}")
                
                if lemma_pos_known:
                    logger.debug(f"‚úÖ is_token_known: –Ω–∞–π–¥–µ–Ω–æ –≤ known_lemma_pos: ({lemma_l}, {pos})")
                else:
                    logger.debug(f"‚ùå is_token_known: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ known_lemma_pos: ({lemma_l}, {pos})")
                
                return lemma_pos_known
        except Exception as e:
            logger.debug(f"‚ùå is_token_known –æ—à–∏–±–∫–∞: {e}")
            return False
    
    def filter_unknown_words(self, words: List[str]) -> List[str]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞.
        
        Args:
            words: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        """
        if not words:
            return []
        
        unknown_words = []
        for word in words:
            if not self.is_word_known(word):
                unknown_words.append(word)
        
        return unknown_words

    # --- –ü–æ–¥—Å–∫–∞–∑–∫–∏ (–Ω–µ –¥–µ–ª–∞—é—Ç —Å–ª–æ–≤–æ ¬´–∏–∑–≤–µ—Å—Ç–Ω—ã–º¬ª) ---
    def get_similar_candidates(self, *, lemma: Optional[str], pos: Optional[str], gender: Optional[str]) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ ¬´–ø–æ—Ö–æ–∂–∏—Ö¬ª –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∏–∑ ANKI –ø–æ –ª–µ–º–º–µ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏.

        –≠—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –≤ –æ—Ç—á—ë—Ç–∞—Ö –∏ –Ω–µ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ç—Ä–æ–≥—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
        –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏. –î–∏–∞–∫—Ä–∏—Ç–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è, –Ω–∏–∫–∞–∫–∏—Ö —Ä—É—á–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ –Ω–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è.
        """
        if not lemma:
            return []
        lemma_l = lemma.lower().strip()
        suggestions: List[str] = []
        
        try:
            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –ª–µ–º–º–æ–π
            if lemma_l in self.known_words:
                suggestions.append(lemma_l)
            
            # 2. –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç, –∏—â–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –ª–µ–º–º—É –∫–∞–∫ –∫–æ—Ä–µ–Ω—å
            if not suggestions:
                # –ò—â–µ–º —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å –Ω–∞—à–µ–π –ª–µ–º–º—ã (–Ω–æ –Ω–µ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è)
                candidates = []
                for known_word in self.known_words:
                    if known_word.startswith(lemma_l) and known_word != lemma_l:
                        candidates.append(known_word)
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ - –±–ª–∏–∂–∞–π—à–∏–µ —Ñ–æ—Ä–º—ã –ø–µ—Ä–≤—ã–º–∏  
                candidates.sort(key=len)
                suggestions.extend(candidates[:3])
            
            # 3. –ï—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–≤—Ä–∞—Ç–Ω—ã–π –≥–ª–∞–≥–æ–ª (—Å 'se'), –∏—â–µ–º –±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É
            if not suggestions and lemma_l.endswith('se') and len(lemma_l) > 3:
                base_verb = lemma_l[:-2]  # —É–±–∏—Ä–∞–µ–º 'se'
                if base_verb in self.known_words:
                    suggestions.append(base_verb)
                else:
                    # –ò—â–µ–º —Ñ–æ—Ä–º—ã –±–∞–∑–æ–≤–æ–≥–æ –≥–ª–∞–≥–æ–ª–∞
                    for known_word in self.known_words:
                        if known_word.startswith(base_verb) and known_word != base_verb:
                            suggestions.append(known_word)
                            if len(suggestions) >= 2:
                                break
            
            # 4. –î–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –ù–ï —Å–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –†–ï–ê–õ–¨–ù–û —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ ANKI
            # spaCy-–∏–Ω–¥–µ–∫—Å—ã –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –æ—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö —Ñ—Ä–∞–∑
            
        except Exception:
            return []
        
        return suggestions[:5]
    
    def get_known_words_count(self) -> int:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤.
        
        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        """
        return len(self.known_words)
    
    def get_known_words_sample(self, n: int = 10) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—Ä–∞–∑–µ—Ü –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤.
        
        Args:
            n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        """
        return list(self.known_words)[:n]
    
    def add_known_word(self, word: str) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–æ –≤ —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö.
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
        """
        if word and isinstance(word, str):
            word_lower = word.lower().strip()
            self.known_words.add(word_lower)
            
            normalized = self.normalizer.normalize(word)
            if normalized:
                self.normalized_known_words.add(normalized)
    
    def remove_known_word(self, word: str) -> None:
        """
        –£–¥–∞–ª—è–µ—Ç —Å–ª–æ–≤–æ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö.
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        """
        if word:
            word_lower = word.lower().strip()
            self.known_words.discard(word_lower)
            
            normalized = self.normalizer.normalize(word)
            if normalized:
                self.normalized_known_words.discard(normalized)
    
    def get_comparison_statistics(self) -> Dict[str, int]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return {
            'known_words_count': len(self.known_words),
            'normalized_known_words_count': len(self.normalized_known_words),
            'collection_path': self.collection_path,
            'deck_pattern': self.deck_pattern
        }
    
    def reload_known_words(self) -> bool:
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ Anki.
        
        Returns:
            True –µ—Å–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
        """
        if self.collection_path and os.path.exists(self.collection_path):
            self.load_known_words(self.collection_path)
            return True
        return False
