"""
–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä spaCy –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞.

–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –ø–∞–π–ø–ª–∞–π–Ω–∞
–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –∫–æ—Ä—Ä–µ–∫—Ü–∏—é POS —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É spacy-pipeline.mdc.
"""

import spacy
import time
import logging
from typing import Optional, List, Dict, Any
from ..config import config

logger = logging.getLogger(__name__)

class SpacyManager:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä spaCy –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞.
    
    –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç:
    - –ï–¥–∏–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
    - –ö–æ—Ä—Ä–µ–∫—Ü–∏—é —Å–∏—Å—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫ POS
    - –ï–¥–∏–Ω–æ–æ–±—Ä–∞–∑–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
    """
    
    _instance: Optional['SpacyManager'] = None
    _nlp: Optional[spacy.Language] = None
    
    def __new__(cls) -> 'SpacyManager':
        """–°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ (–≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ä–∞–∑)."""
        if self._nlp is None:
            self._load_model()
    
    def _load_model(self) -> None:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å spaCy —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É."""
        model_name = config.get_spacy_model()
        
        # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª—è—Ö —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º UI
        if 'trf' in model_name:
            print("‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç—è–∂—ë–ª–∞—è –º–æ–¥–µ–ª—å spaCy (transformer)")
            print("üí° –î–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ es_core_news_md –≤ config.yaml")
        
        try:
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞: –∏—Å–∫–ª—é—á–∞–µ–º NER (—Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É spacy-pipeline)
            exclude_components = ['ner']  # NER –Ω–µ –Ω—É–∂–µ–Ω –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            
            self._nlp = spacy.load(model_name, exclude=exclude_components)
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç –¥–ª—è —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ POS
            if not spacy.tokens.Token.has_extension("corrected_pos"):
                spacy.tokens.Token.set_extension("corrected_pos", default=None)
            
            performance_note = " (–º–µ–¥–ª–µ–Ω–Ω–∞—è, –Ω–æ —Ç–æ—á–Ω–∞—è)" if 'trf' in model_name else " (–±—ã—Å—Ç—Ä–∞—è)"
            print(f"‚úÖ SpaCy –º–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞{performance_note} (–∏—Å–∫–ª—é—á–µ–Ω—ã: {exclude_components})")
            
        except Exception as e:
            raise RuntimeError(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å spaCy '{model_name}'. "
                f"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –º–æ–¥–µ–ª—å: python -m spacy download {model_name}"
            ) from e
    
    def get_nlp(self) -> spacy.Language:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å spaCy."""
        if self._nlp is None:
            self._load_model()
        return self._nlp
    
    def analyze_text_with_corrections(self, text: str) -> spacy.tokens.Doc:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –∫–æ—Ä—Ä–µ–∫—Ü–∏–π POS —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            spaCy Doc —Å —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ POS-—Ç–µ–≥–∞–º–∏ (—á–µ—Ä–µ–∑ –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ)
        """
        t0 = time.time()
        nlp = self.get_nlp()
        # –õ—ë–≥–∫–∏–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–ª–æ–≥: –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –º–æ–¥–µ–ª—å
        logger.debug(f"spaCy –∞–Ω–∞–ª–∏–∑: —Å—Ç–∞—Ä—Ç (len={len(text)} —Å–∏–º–≤–æ–ª–æ–≤, model={config.get_spacy_model()}, pipeline={nlp.pipe_names})")
        doc = nlp(text)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ POS –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–∞—Ö
        for token in doc:
            corrected_pos = self._correct_pos_tag(token, doc)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π POS –≤ –∫–∞—Å—Ç–æ–º–Ω–æ–º –∞—Ç—Ä–∏–±—É—Ç–µ
            # —Ç–∞–∫ –∫–∞–∫ –ø—Ä—è–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ token.pos_ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥—ë–∂–Ω–æ
            token._.corrected_pos = corrected_pos
        dt = time.time() - t0
        logger.debug(f"spaCy –∞–Ω–∞–ª–∏–∑: –∑–∞–≤–µ—Ä—à—ë–Ω (tokens={len(doc)}, time={dt:.2f}s)")
        
        return doc
    
    def _correct_pos_tag(self, token: spacy.tokens.Token, doc: spacy.tokens.Doc) -> str:
        """
        –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç POS-—Ç–µ–≥ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª–∞–º spacy-pipeline.mdc.
        
        Args:
            token: –¢–æ–∫–µ–Ω –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
            doc: –î–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π POS-—Ç–µ–≥
        """
        pos_tag = token.pos_
        
        # 1. PROPN –∫–æ—Ä—Ä–µ–∫—Ü–∏—è: –í–°–ï PROPN ‚Üí NOUN (–±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏)
        if pos_tag == 'PROPN':
            return 'NOUN'  # –†–µ–º–∞–ø PROPN ‚Üí NOUN –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
        
        # 2. SYM —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º None –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è)
        if pos_tag == 'SYM' and token.is_alpha and len(token.text) > 2:
            # –û—à–∏–±–∫–∞ —Ä–∞–∑–º–µ—Ç–∫–∏ - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–æ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            return 'X'  # –ü–æ–º–µ—á–∞–µ–º –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        
        return pos_tag
    
    def _is_proper_noun(self, text: str) -> bool:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–ª–æ–≤–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–º –∏–º–µ–Ω–µ–º.
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è
        """
        word_lower = text.lower()
        
        # –ú–∞—Ä–∫–µ—Ä—ã —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–º—ë–Ω
        proper_noun_indicators = [
            # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
            lambda w: w in ['espa√±a', 'madrid', 'barcelona', 'valencia', 'sevilla', 'andaluc√≠a'],
            # –ò–º–µ–Ω–∞ –ª—é–¥–µ–π (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
            lambda w: len(w) <= 8 and any(w.endswith(suffix) for suffix in ['o', 'a', 'ez', 'es']) and w.istitle(),
            # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ –±—Ä–µ–Ω–¥—ã
            lambda w: w in ['google', 'microsoft', 'toyota', 'samsung', 'apple', 'amazon'],
            # –ú–µ—Å—è—Ü—ã –∏ –¥–Ω–∏ –Ω–µ–¥–µ–ª–∏
            lambda w: w in ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio',
                          'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre',
                          'lunes', 'martes', 'mi√©rcoles', 'jueves', 'viernes', 's√°bado', 'domingo']
        ]
        
        for indicator in proper_noun_indicators:
            try:
                if indicator(word_lower):
                    return True
            except:
                continue
        
        return False
    
    def get_quality_statistics(self, doc: spacy.tokens.Doc) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É.
        
        Args:
            doc: –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
        """
        total_tokens = len([t for t in doc if t.is_alpha])
        x_sym_tokens = len([t for t in doc if t.is_alpha and t.pos_ in ['X', 'SYM']])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∞—Å—Ç—è–º —Ä–µ—á–∏
        pos_distribution = {}
        for token in doc:
            if token.is_alpha:
                pos = token.pos_
                pos_distribution[pos] = pos_distribution.get(pos, 0) + 1
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
        main_pos_count = sum(pos_distribution.get(pos, 0) for pos in ['NOUN', 'VERB', 'ADJ'])
        main_pos_ratio = main_pos_count / total_tokens if total_tokens > 0 else 0
        
        return {
            'total_alpha_tokens': total_tokens,
            'x_sym_tokens': x_sym_tokens,
            'x_sym_ratio': x_sym_tokens / total_tokens if total_tokens > 0 else 0,
            'main_pos_ratio': main_pos_ratio,
            'pos_distribution': pos_distribution,
            'quality_warnings': self._get_quality_warnings(total_tokens, x_sym_tokens, main_pos_ratio)
        }
    
    def _get_quality_warnings(self, total_tokens: int, x_sym_tokens: int, main_pos_ratio: float) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∞–≤–∏–ª—É."""
        warnings = []
        
        if total_tokens > 0:
            x_sym_ratio = x_sym_tokens / total_tokens
            
            if x_sym_ratio > 0.15:
                warnings.append(f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–Ω–æ: {x_sym_ratio:.1%} —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–ª—É—á–∏–ª–∏ X/SYM - —Å–µ—Ä—å—ë–∑–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–µ–∫—Å—Ç–æ–º")
            elif x_sym_ratio > 0.10:
                warnings.append(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {x_sym_ratio:.1%} —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–ª—É—á–∏–ª–∏ X/SYM - –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–µ–∫—Å—Ç–æ–º")
            
            if main_pos_ratio < 0.60:
                warnings.append(f"‚ö†Ô∏è –ù–∏–∑–∫–∞—è –¥–æ–ª—è –æ—Å–Ω–æ–≤–Ω—ã—Ö POS: {main_pos_ratio:.1%} (–æ–∂–∏–¥–∞–µ—Ç—Å—è > 60%)")
        
        return warnings
    
    def reload_model(self) -> bool:
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å spaCy.
        
        Returns:
            True –µ—Å–ª–∏ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞
        """
        try:
            self._nlp = None
            self._load_model()
            return True
        except Exception:
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        if self._nlp is None:
            return {'loaded': False}
        
        return {
            'name': config.get_spacy_model(),
            'loaded': True,
            'lang': self._nlp.lang,
            'pipeline': self._nlp.pipe_names,
            'excluded_components': ['ner']  # –°–æ–≥–ª–∞—Å–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        }
