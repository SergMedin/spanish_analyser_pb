"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö —Å–ª–æ–≤

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è:
- –ü–æ–¥—Å—á—ë—Ç–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ —Å–ª–æ–≤ –ø–æ —Ç–µ–º–∞–º
- –ê–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é spaCy
- –°–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á—ë—Ç–æ–≤
"""

import json
import pandas as pd
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
import spacy
from .config import config
from .components.word_comparator import WordComparator
from .components.pos_tagger import POSTagger
from .components.tokenizer import TokenProcessor
from .components.lemmatizer import LemmaProcessor
from .components.frequency_analyzer import FrequencyAnalyzer
from .components.normalizer import WordNormalizer
from .components.exporter import ResultExporter
from .interfaces.text_processor import AnalysisResult, WordInfo
import logging

logger = logging.getLogger(__name__)


class WordAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö —Å–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º spaCy"""
    
    def __init__(self,
                 collection_path: Optional[str] = None,
                 deck_pattern: str = "Spanish*",
                 min_word_length: Optional[int] = None,
                 spacy_model: Optional[str] = None,
                 output_dir: Optional[str] = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å–ª–æ–≤"""
        self.word_frequencies = Counter()
        self.word_categories = defaultdict(list)
        self.known_words = set()
        self.word_pos_tags = {}  # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
        self.word_comparator: Optional[WordComparator] = None  # –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ANKI
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –ø–æ (lemma, pos, gender) –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ lemma
        self.token_details = {}  # Dict[(lemma, pos, gender), TokenDetails]
        self.pos_tagger = POSTagger(model_name=config.get_spacy_model())  # –ï–¥–∏–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ POS‚ÜíRU
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ –±–µ—Ä—ë—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏, —á—Ç–æ–±—ã —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ
        # —É–ø—Ä–∞–≤–ª—è—Ç—å –ø–æ—Ä–æ–≥–æ–º —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ –≤–æ –≤—Å–µ—Ö —á–∞—Å—Ç—è—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        # –≠—Ç–æ –≤–∞–∂–Ω–æ, —Ç.–∫. –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ spaCy –º—ã –¥–æ–±–∞–≤–ª—è–µ–º –≤ —á–∞—Å—Ç–æ—Ç—ã –õ–ï–ú–ú–£,
        # –∫–æ—Ç–æ—Ä–∞—è –º–æ–∂–µ—Ç –±—ã—Ç—å –∫–æ—Ä–æ—á–µ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´vamos¬ª ‚Üí ¬´ir¬ª),
        # –∏ –∏–º–µ–Ω–Ω–æ –¥–ª–∏–Ω—É –ª–µ–º–º—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Å –ø–æ—Ä–æ–≥–æ–º.
        self.min_word_length = min_word_length or config.get_min_word_length()
        
        # –°–ø–∏—Å–æ–∫ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∞—Ä—Ç–∏–∫–ª–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        self.spanish_articles = {
            'el', 'la', 'los', 'las',  # –û–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫–ª–∏
            'un', 'una', 'unos', 'unas',  # –ù–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∞—Ä—Ç–∏–∫–ª–∏
            'este', 'esta', 'estos', 'estas',  # –£–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            'ese', 'esa', 'esos', 'esas',
            'aquel', 'aquella', 'aquellos', 'aquellas',
            'mi', 'tu', 'su', 'nuestro', 'vuestro',  # –ü—Ä–∏—Ç—è–∂–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            'mi', 'mis', 'tu', 'tus', 'su', 'sus',
            'nuestro', 'nuestra', 'nuestros', 'nuestras',
            'vuestro', 'vuestra', 'vuestros', 'vuestras'
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å spaCy –¥–ª—è –∏—Å–ø–∞–Ω—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        spacy_model = spacy_model or config.get_spacy_model()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä spaCy (—Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥—É)
        from .components.spacy_manager import SpacyManager
        self.spacy_manager = SpacyManager()
        self.nlp = self.spacy_manager.get_nlp()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–≥–æ API)
        self.tokenizer = TokenProcessor(min_length=self.min_word_length, include_numbers=False)
        self.lemmatizer = LemmaProcessor(model_name=spacy_model, use_cache=True, text_model=None)
        # pos_tagger —É–∂–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –≤—ã—à–µ
        self.frequency_analyzer = FrequencyAnalyzer()
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º WordComparator, –µ—Å–ª–∏ –µ—â—ë –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ init_anki_integration
        if self.word_comparator is None:
            try:
                # –£–±–∏—Ä–∞–µ–º –∑–≤–µ–∑–¥–æ—á–∫—É –∏–∑ deck_pattern –¥–ª—è WordComparator
                clean_deck_pattern = deck_pattern.rstrip('*')
                # –ù–µ –∑–∞–≥—Ä—É–∂–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ ‚Äî —Å–¥–µ–ª–∞–µ–º —ç—Ç–æ —è–≤–Ω–æ –≤ init_anki_integration()
                self.word_comparator = WordComparator(collection_path=collection_path, deck_pattern=clean_deck_pattern, autoload=False)
            except Exception:
                clean_deck_pattern = deck_pattern.rstrip('*')
                self.word_comparator = WordComparator(collection_path=collection_path, deck_pattern=clean_deck_pattern, autoload=False)
        self.word_normalizer = WordNormalizer(use_cache=True)
        self.exporter = ResultExporter(output_dir=output_dir or config.get_results_folder())
        # –î–ª—è –Ω–æ–≤–æ–≥–æ API –æ–∂–∏–¥–∞—é—Ç—Å—è —ç—Ç–∏ –ø–æ–ª—è
        self.spacy_model = spacy_model
        
        # –£–î–ê–õ–ï–ù–û: POS_NAMES —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ–¥–∏–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏–∑ POSTagger.get_pos_tag_ru

    # ===== –ú–µ—Ç–æ–¥—ã –Ω–æ–≤–æ–≥–æ API (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å word_analyzer_new.py) =====

    def analyze_text(self, text: str) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–Ω–æ–≤—ã–π API)."""
        import time as _time
        start = _time.time()
        if not text or not text.strip():
            return self._create_empty_result()

        tokens = self.tokenizer.tokenize(text)
        if not tokens:
            return self._create_empty_result()

        lemmas = self.lemmatizer.lemmatize_batch(tokens)
        pos_tags = self.pos_tagger.get_pos_tags(tokens)
        pos_tags_ru = [self.pos_tagger.get_pos_tag_ru(p) for p in pos_tags]
        try:
            genders = self.pos_tagger.get_genders(tokens)
        except Exception:
            genders = [None] * len(tokens)

        # –ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º –Ω–æ–≤–æ–≥–æ API
        counters_tokens: List[str] = []
        for lemma, pos, gender in zip(lemmas, pos_tags, genders):
            key = lemma
            if pos == 'NOUN':
                art = 'el' if gender == 'Masc' else ('la' if gender == 'Fem' else '')
                key = f"{art + ' ' if art else ''}{lemma}"
            counters_tokens.append(key)
        frequency_dict = self.frequency_analyzer.count_frequency(counters_tokens)

        words_info: List[WordInfo] = []
        for tok, lemma, pos, pos_ru, gender in zip(tokens, lemmas, pos_tags, pos_tags_ru, genders):
            if pos == 'NOUN':
                art = 'el' if gender == 'Masc' else ('la' if gender == 'Fem' else '')
                freq_key = f"{art + ' ' if art else ''}{lemma}"
            else:
                freq_key = lemma
            freq = frequency_dict.get(freq_key, 0)
            is_known = False
            try:
                if self.word_comparator:
                    if config.is_lemma_aware_known_enabled():
                        is_known = self.word_comparator.is_token_known(lemma=lemma, pos=pos, gender=gender)
                        logger.debug(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ LEMMA-—Ä–µ–∂–∏–º: '{lemma}' (pos={pos}, gender={gender}) ‚Üí {is_known}")
                    else:
                        is_known = self.word_comparator.is_word_known(tok)
                        logger.debug(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ –¢–û–ß–ù–û–ï-—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: '{tok}' ‚Üí {is_known}")
                else:
                    logger.debug(f"‚ö†Ô∏è WordComparator –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è —Å–ª–æ–≤–∞: '{tok}'")
            except Exception as e:
                logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è '{tok}': {e}")
                pass
            words_info.append(WordInfo(
                word=tok,
                pos_tag=pos,
                pos_tag_ru=pos_ru,
                frequency=freq,
                lemma=lemma,
                is_known=is_known,
                context_examples=None,
                comment=None,
                gender=gender
            ))

        processing_time = _time.time() - start
        return AnalysisResult(
            words=words_info,
            frequency_dict=frequency_dict,
            unknown_words=[w.word for w in words_info if not w.is_known],
            total_words=len(tokens),
            unique_words=len(frequency_dict),
            processing_time=processing_time,
            metadata={
                'spacy_model': getattr(self, 'spacy_model', config.get_spacy_model()),
                'min_word_length': self.min_word_length,
                'collection_path': getattr(self.word_comparator, 'collection_path', ''),
                'deck_pattern': getattr(self.word_comparator, 'deck_pattern', ''),
            }
        )

    def get_unknown_words_for_learning(self, result: AnalysisResult) -> List[WordInfo]:
        if not result or not result.words:
            return []
        unknown = [w for w in result.words if not w.is_known]
        def _prio(w: WordInfo):
            return (w.frequency, self.pos_tagger.get_learning_priority(w.pos_tag))
        unknown.sort(key=_prio, reverse=True)
        return unknown

    def export_results(self, result: AnalysisResult, base_filename: str = "spanish_analysis"):
        return self.exporter.export_all_formats(result, base_filename)

    def get_statistics(self) -> Dict[str, Any]:
        return {
            'tokenizer': self.tokenizer.get_token_statistics([]),
            'lemmatizer': self.lemmatizer.get_cache_stats(),
            'pos_tagger': {'model_loaded': True, 'model_name': self.pos_tagger.model_name},
            'frequency_analyzer': self.frequency_analyzer.get_frequency_statistics(),
            'word_comparator': (self.word_comparator.get_comparison_statistics() if self.word_comparator else {}),
            'word_normalizer': self.word_normalizer.get_cache_stats(),
            'settings': {
                'min_word_length': self.min_word_length,
                'spacy_model': getattr(self, 'spacy_model', config.get_spacy_model()),
                'output_dir': config.get_results_folder()
            }
        }

    def clear_caches(self) -> None:
        try:
            self.lemmatizer.clear_cache()
            self.word_normalizer.clear_cache()
            self.frequency_analyzer.reset_statistics()
        except Exception:
            pass

    def reload_models(self) -> bool:
        ok1 = False
        ok2 = False
        try:
            ok1 = self.lemmatizer.reload_model()
        except Exception:
            pass
        try:
            ok2 = self.pos_tagger.reload_model()
        except Exception:
            pass
        return bool(ok1 and ok2)

    def _create_empty_result(self) -> AnalysisResult:
        return AnalysisResult(words=[], frequency_dict={}, unknown_words=[], total_words=0, unique_words=0, processing_time=0.0)

    # –ú–µ—Ç–æ–¥—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –Ω–æ–≤–æ–≥–æ API
    def analyze_spanish_text(self, text: str) -> Dict[str, Any]:
        res = self.analyze_text(text)
        return {
            'words': [w.word for w in res.words],
            'frequencies': res.frequency_dict,
            'unknown_words': res.unknown_words,
            'total_words': res.total_words,
            'unique_words': res.unique_words,
            'processing_time': res.processing_time,
            'pos_tags': {w.word: w.pos_tag_ru for w in res.words},
        }

    def get_word_frequency(self, word: str) -> int:
        return self.frequency_analyzer.get_word_frequency(word)

    def get_most_frequent_words(self, n: int = 10) -> List[tuple]:
        return self.frequency_analyzer.get_most_frequent(n)

    # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ç–µ—Å—Ç–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞: fallback-–º–µ—Ç–æ–¥
    def determine_pos(self, lemma: str) -> str:
        try:
            tag = self.pos_tagger.get_pos_tags([lemma])[0]
            return self.pos_tagger.get_pos_tag_ru(tag).lower()
        except Exception:
            return '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
    
    def normalize_word(self, word: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —á–µ—Ä–µ–∑ WordNormalizer (spaCy)."""
        return self.word_normalizer.normalize(word)
    
    def is_word_known(self, word: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∏–∑–≤–µ—Å—Ç–Ω–æ –ª–∏ —Å–ª–æ–≤–æ —á–µ—Ä–µ–∑ WordComparator –∏–ª–∏ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —Å–ª–æ–≤–æ –∏–∑–≤–µ—Å—Ç–Ω–æ, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        if not word:
            return False
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤ –∏ legacy-–ª–æ–≥–∏–∫–∏)
        normalized_word = self.normalize_word(word)
        if normalized_word in self.known_words:
            return True

        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω WordComparator, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if self.word_comparator:
            if self.word_comparator.is_word_known(word):
                return True
        
        # –ò–Ω–∞—á–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        if not self.known_words:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if normalized_word in self.known_words:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ–æ—Ä–º—ã
        if hasattr(self, '_normalized_known_words'):
            if normalized_word in self._normalized_known_words:
                return True
        else:
            # –°–æ–∑–¥–∞—ë–º –∫—ç—à –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –≤—ã–∑–æ–≤–µ
            self._normalized_known_words = set()
            for known_word in self.known_words:
                normalized_known = self.normalize_word(known_word)
                if normalized_known:
                    self._normalized_known_words.add(normalized_known)
            
            # –¢–µ–ø–µ—Ä—å –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ –∫—ç—à–µ
            if normalized_word in self._normalized_known_words:
                return True
        
        return False
    
    # –£–î–ê–õ–ï–ù–û: —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–µ—Ç–æ–¥—ã POS-–∞–Ω–∞–ª–∏–∑–∞
    # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π SpacyManager
    
    def init_anki_integration(self) -> bool:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å ANKI —á–µ—Ä–µ–∑ AnkiConnect.
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞
        """
        try:
            if self.word_comparator is None:
                # –°–æ–∑–¥–∞–¥–∏–º –∏ –∑–∞–≥—Ä—É–∑–∏–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
                self.word_comparator = WordComparator(deck_pattern="Spanish", autoload=True)
            elif self.word_comparator.get_known_words_count() == 0:
                # –û—Ç–ª–æ–∂–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞, –µ—Å–ª–∏ –∫–æ–º–ø–∞—Ä–∞—Ç–æ—Ä —É–∂–µ —Å–æ–∑–¥–∞–Ω
                try:
                    # –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ AnkiConnect
                    self.word_comparator._load_known_words_modern()  # type: ignore[attr-defined]
                except Exception:
                    # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–æ–∫ ‚Äî –ø–µ—Ä–µ—Å–æ–∑–¥–∞–¥–∏–º —Å –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫–æ–π
                    self.word_comparator = WordComparator(deck_pattern="Spanish", autoload=True)
            known_count = self.word_comparator.get_known_words_count()
            if known_count > 0:
                # –û–±–Ω–æ–≤–ª—è–µ–º deprecated –ø–æ–ª–µ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                self.known_words = set(self.word_comparator.known_words)
                logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å ANKI: {known_count} –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤")
                return True
            else:
                logger.warning("ANKI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ—Ç –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥")
                return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ANKI: {e}")
            return False
    
    # –£–î–ê–õ–ï–ù–û: —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –º–µ—Ç–æ–¥—ã determine_pos_* –∑–∞–º–µ–Ω–µ–Ω—ã –Ω–∞ SpacyManager
    
    def load_known_words_from_anki(self, anki_integration: Any, deck_pattern: str = None, field_names: List[str] = None) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∫–æ–ª–æ–¥ Anki
        
        Args:
            anki_integration: –≠–∫–∑–µ–º–ø–ª—è—Ä AnkiIntegration
            deck_pattern: –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–ª–æ–¥ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
            field_names: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ª–æ–≤ (–µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏)
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        deck_pattern = deck_pattern or config.get_deck_pattern()
        field_names = field_names or config.get_field_names()
        
        try:
            if not anki_integration.is_connected():
                logger.error("–ù–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Anki –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤")
                return False
            
            # –ù–∞—Ö–æ–¥–∏–º –∑–∞–º–µ—Ç–∫–∏ –≤ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥–∞—Ö
            note_ids = anki_integration.find_notes_by_deck(deck_pattern)
            if not note_ids:
                logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –∑–∞–º–µ—Ç–æ–∫ –≤ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥–∞—Ö –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É: {deck_pattern}")
                return False
            
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(note_ids)} –∑–∞–º–µ—Ç–æ–∫ –≤ –∏—Å–ø–∞–Ω—Å–∫–∏—Ö –∫–æ–ª–æ–¥–∞—Ö")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –∑–∞–º–µ—Ç–æ–∫
            notes_data = anki_integration.extract_text_from_notes(note_ids, field_names)
            
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
            all_words = set()
            normalized_words = set()
            
            for note_data in notes_data:
                for text in note_data['texts']:
                    if text:
                        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç HTML –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞
                        from .text_processor import SpanishTextProcessor
                        processor = SpanishTextProcessor()
                        cleaned_text = processor.clean_text(text, remove_prefixes=False)
                        words = processor.extract_spanish_words(cleaned_text)
                        
                        for word in words:
                            if word.strip():
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
                                all_words.add(word.strip())
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ
                                normalized = self.normalize_word(word)
                                if normalized:
                                    normalized_words.add(normalized)
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ, –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)
            self.known_words = all_words | normalized_words
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_words)} –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ Anki")
            logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(normalized_words)} –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º")
            logger.info(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: {len(self.known_words)}")
            
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ Anki: {e}")
            logger.info("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Anki –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∞ –∫–æ–ª–æ–¥ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return False
    
    def load_known_words(self, file_path: str) -> bool:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π –º–µ—Ç–æ–¥)
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            
        Returns:
            True –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —É—Å–ø–µ—à–Ω–∞, False –≤ –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ
        """
        logger.warning("–ú–µ—Ç–æ–¥ load_known_words() —É—Å—Ç–∞—Ä–µ–ª. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ load_known_words_from_anki()")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                words = [line.strip().lower() for line in f if line.strip()]
                self.known_words = set(words)
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(words)} –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤: {e}")
            return False
    
    def add_words_from_text(self, text: str, weight: int = 1) -> None:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —á–∞—Å—Ç–æ—Ç —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —á–∞—Å—Ç–µ–π —Ä–µ—á–∏
        (–æ–±–Ω–æ–≤–ª—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ä—Ç–∏–∫–ª–µ–π –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö)
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            weight: –í–µ—Å –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1)
        """
        if not text:
            return
        
        if not self.nlp:
            raise RuntimeError("–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω: –º–æ–¥–µ–ª—å spaCy –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        try:
            import time as _time
            _t0 = _time.time()
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤: –∞–Ω–∞–ª–∏–∑ spaCy (len={len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –∫–æ—Ä—Ä–µ–∫—Ü–∏–π (—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞)
            doc = self.spacy_manager.analyze_text_with_corrections(text)
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤: spaCy doc –≥–æ—Ç–æ–≤ (tokens={len(doc)}, dt={_time.time()-_t0:.2f}s)")
            
            # === –¶–ï–ù–¢–†–ê–õ–ò–ó–û–í–ê–ù–ù–´–ï –≠–í–†–ò–°–¢–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê ===
            quality_stats = self.spacy_manager.get_quality_statistics(doc)
            for warning in quality_stats['quality_warnings']:
                logger.warning(warning)
            
            _t1 = _time.time()
            added_tokens = 0
            for token in doc:
                if not token.is_alpha:
                    continue
                lemma = token.lemma_.lower()
                
                # === –ò–°–ü–û–õ–¨–ó–£–ï–ú –°–ö–û–†–†–ï–ö–¢–ò–†–û–í–ê–ù–ù–´–ô POS ===
                # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π POS –∏–∑ SpacyManager, –∏–Ω–∞—á–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π
                pos_tag = getattr(token._, 'corrected_pos', None) or token.pos_
                
                # === –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û–°–õ–ï –ö–û–†–†–ï–ö–¶–ò–ô ===
                # –ö–æ—Ä—Ä–µ–∫—Ü–∏–∏ —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –≤ SpacyManager, —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
                if pos_tag == 'X' and token.is_alpha and len(token.text) > 2:
                    # –¢–æ–∫–µ–Ω –ø–æ–º–µ—á–µ–Ω –∫–∞–∫ –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –≤ SpacyManager - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    continue
                elif pos_tag == 'SYM':
                    # –û—Å—Ç–∞–≤—à–∏–µ—Å—è SYM —Ç–æ–∫–µ–Ω—ã –∏—Å–∫–ª—é—á–∞–µ–º –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–≤
                    continue
                
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—É —Å –≤–æ–∑–≤—Ä–∞—Ç–Ω—ã–º–∏ –≥–ª–∞–≥–æ–ª–∞–º–∏
                # spaCy –¥–∞—ë—Ç –ª–µ–º–º—É "detener √©l" –¥–ª—è "detenerse", –Ω–æ –Ω–∞–º –Ω—É–∂–Ω–æ "detenerse"
                if pos_tag in ['VERB', 'AUX'] and lemma.endswith(' √©l'):
                    # –ï—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ 'se', —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ—ë
                    original_text = token.text.lower()
                    if original_text.endswith('se'):
                        lemma = original_text
                    else:
                        # –ò–Ω–∞—á–µ —É–±–∏—Ä–∞–µ–º ' √©l' –∏–∑ –ª–µ–º–º—ã
                        lemma = lemma.replace(' √©l', '')
                
                if len(lemma) < self.min_word_length:
                    continue
                pos_name = self.pos_tagger.get_pos_tag_ru(pos_tag)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)
                gender = None
                if token.morph:
                    # –ü–∞—Ä—Å–∏–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                    morph_dict = {}
                    for feature in token.morph:
                        if '=' in feature:
                            key, value = feature.split('=', 1)
                            morph_dict[key] = value.split(',')
                    if 'Gender' in morph_dict:
                        gender = morph_dict['Gender'][0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ

                # –†–∞–Ω–Ω–µ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä–æ–¥–∞ NOUN –ø–æ –±–ª–∏–∂–∞–π—à–µ–º—É DET (el/la/los/las)
                if pos_tag == 'NOUN' and not gender:
                    try:
                        # –ò—â–µ–º –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–∏–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ–ª—å
                        if token.i > 0:
                            det = doc[token.i - 1]
                            if det.pos_ == 'DET':
                                art = det.lemma_.lower() if det.lemma_ else det.text.lower()
                                if art in ('el', 'los'):
                                    gender = 'Masc'
                                elif art in ('la', 'las'):
                                    gender = 'Fem'
                        # –í —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
                        if not gender and token.i + 1 < len(doc):
                            det_next = doc[token.i + 1]
                            if det_next.pos_ == 'DET':
                                art = det_next.lemma_.lower() if det_next.lemma_ else det_next.text.lower()
                                if art in ('el', 'los'):
                                    gender = 'Masc'
                                elif art in ('la', 'las'):
                                    gender = 'Fem'
                    except Exception:
                        pass
                
                # –î–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á —Å –∞—Ä—Ç–∏–∫–ª–µ–º
                if pos_tag == 'NOUN':
                    if gender == 'Masc':
                        display_word = f"el {lemma}"
                    elif gender == 'Fem':
                        display_word = f"la {lemma}"
                    else:
                        display_word = lemma  # –ë–µ–∑ –∞—Ä—Ç–∏–∫–ª—è –µ—Å–ª–∏ —Ä–æ–¥ –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω
                    freq_key = f"{display_word} ({pos_name})"
                else:
                    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–º–º—É –∫–∞–∫ –µ—Å—Ç—å
                    freq_key = f"{lemma} ({pos_name})"
                
                self.word_frequencies[freq_key] += weight
                self.word_pos_tags[lemma] = pos_name.lower()
                
                # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –ø–æ (lemma, pos, gender)
                safe_key = (lemma, pos_tag, gender)
                self.token_details[safe_key] = {
                    'pos': pos_tag,
                    'pos_ru': pos_name,
                    'gender': gender,
                    'original_text': token.text,
                    'display_form': display_word if pos_tag == 'NOUN' else lemma,
                    'freq_key': freq_key  # –ö–ª—é—á –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
                }
                added_tokens += 1
                
            logger.debug(f"–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ª–æ–≤: –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (added={added_tokens}, dt={_time.time()-_t1:.2f}s)")
        except Exception as e:
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞ —Å spaCy: {e}")
    
    def _add_words_basic(self, text: str, weight: int = 1) -> None:
        raise RuntimeError("–ë–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ spaCy –æ—Ç–∫–ª—é—á–µ–Ω–∞ –ø–æ–ª–∏—Ç–∏–∫–æ–π No Fallback")
    
    def categorize_words_by_frequency(self, min_frequency: int = 1) -> Dict[str, List[str]]:
        """
        –ö–∞—Ç–µ–≥–æ—Ä–∏–∑—É–µ—Ç —Å–ª–æ–≤–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        
        Args:
            min_frequency: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏ —Å–ª–æ–≤
        """
        categories = {
            '–æ—á–µ–Ω—å_—á–∞—Å—Ç–æ': [],      # > 100 —Ä–∞–∑
            '—á–∞—Å—Ç–æ': [],            # 50-100 —Ä–∞–∑
            '—Å—Ä–µ–¥–Ω–µ': [],           # 20-49 —Ä–∞–∑
            '—Ä–µ–¥–∫–æ': [],            # 5-19 —Ä–∞–∑
            '–æ—á–µ–Ω—å_—Ä–µ–¥–∫–æ': []       # 1-4 —Ä–∞–∑–∞
        }
        
        for word_with_pos, freq in self.word_frequencies.most_common():
            if freq < min_frequency:
                continue
                
            if freq > 100:
                categories['–æ—á–µ–Ω—å_—á–∞—Å—Ç–æ'].append(word_with_pos)
            elif freq > 50:
                categories['—á–∞—Å—Ç–æ'].append(word_with_pos)
            elif freq > 20:
                categories['—Å—Ä–µ–¥–Ω–µ'].append(word_with_pos)
            elif freq > 5:
                categories['—Ä–µ–¥–∫–æ'].append(word_with_pos)
            else:
                categories['–æ—á–µ–Ω—å_—Ä–µ–¥–∫–æ'].append(word_with_pos)
        
        return categories
    
    def get_new_words(self, exclude_known: bool = True) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö) —Å–ª–æ–≤
        
        Args:
            exclude_known: –ò—Å–∫–ª—é—á–∞—Ç—å –ª–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Å–ª–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        """
        if exclude_known:
            new_words = []
            for word_with_pos, freq in self.word_frequencies.most_common():
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–æ –±–µ–∑ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
                if ' (' in word_with_pos and word_with_pos.endswith(')'):
                    word = word_with_pos.split(' (')[0]
                else:
                    word = word_with_pos
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å
                if not self.is_word_known(word):
                    new_words.append(word_with_pos)
        else:
            new_words = [word_with_pos for word_with_pos, freq in self.word_frequencies.most_common()]
        
        return new_words
    
    def get_top_words(self, n: int = 50, exclude_known: bool = True) -> List[Tuple[str, int]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ç–æ–ø N —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        
        Args:
            n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞
            exclude_known: –ò—Å–∫–ª—é—á–∞—Ç—å –ª–∏ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Å–ª–æ–≤–æ, —á–∞—Å—Ç–æ—Ç–∞)
        """
        if exclude_known:
            top_words = []
            for word_with_pos, freq in self.word_frequencies.most_common():
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–æ –±–µ–∑ —á–∞—Å—Ç–∏ —Ä–µ—á–∏
                if ' (' in word_with_pos and word_with_pos.endswith(')'):
                    word = word_with_pos.split(' (')[0]
                else:
                    word = word_with_pos
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ –∏–∑–≤–µ—Å—Ç–Ω–æ—Å—Ç—å
                if not self.is_word_known(word):
                    top_words.append((word_with_pos, freq))
        else:
            top_words = [(word_with_pos, freq) for word_with_pos, freq in self.word_frequencies.most_common()]
        
        return top_words[:n]
    
    def export_to_excel(self, file_path: str, include_categories: bool = True) -> None:
        """
        –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–ª–æ–≤ –≤ Excel —Ñ–∞–π–ª —Å –∞—Ä—Ç–∏–∫–ª—è–º–∏ –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ ANKI
        
        Args:
            file_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è Excel —Ñ–∞–π–ª–∞
            include_categories: –ü–∞—Ä–∞–º–µ—Ç—Ä –æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏, –Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        """
        try:
            import time as _time
            _t0 = _time.time()
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Å–µ—Ö —Å–ª–æ–≤ (–≤–∫–ª—é—á–∞—è –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è)
            total_words = sum(self.word_frequencies.values())
            logger.debug(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel: —Å—Ç–∞—Ä—Ç (total_words={total_words}, unique={len(self.word_frequencies)})")
            
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            decimal_places = config.get_frequency_decimal_places()
            sheet_name = config.get_main_sheet_name()
            
            # === –£–ú–ù–ê–Ø –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–Ø –ü–û –õ–ï–ú–ú–ê–ú (–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è) ===
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ token_details, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ spaCy
            lemma_pos_analysis = {}  # lemma -> {pos_ru: {freq_key: freq, total_count: int}}
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –†–ï–ê–õ–¨–ù–´–ú –ª–µ–º–º–∞–º –∏–∑ spaCy (–∏–∑ token_details)
            for safe_key, details in self.token_details.items():
                lemma, pos_tag, gender = safe_key
                freq_key = details['freq_key']
                pos_ru = details['pos_ru']
                
                # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –∏–∑ word_frequencies
                freq = self.word_frequencies.get(freq_key, 0)
                if freq == 0:
                    continue
                
                if lemma not in lemma_pos_analysis:
                    lemma_pos_analysis[lemma] = {}
                
                if pos_ru not in lemma_pos_analysis[lemma]:
                    lemma_pos_analysis[lemma][pos_ru] = {'entries': {}, 'total_count': 0}
                
                lemma_pos_analysis[lemma][pos_ru]['entries'][freq_key] = freq
                lemma_pos_analysis[lemma][pos_ru]['total_count'] += freq
            
            # –®–∞–≥ 1: –ø—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—É—é –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—é
            # –ú–µ–ª–∫–∏–µ —Ç–∏–ø—ã —Ä–µ—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–æ–±–∞–≤–ª—è–µ–º –∫ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–º—É —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É
            minor_pos_types = {'–°–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–º—è', '–ú–µ–∂–¥–æ–º–µ—Ç–∏–µ', '–ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ', '–ó–Ω–∞–∫ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è', '–°–∏–º–≤–æ–ª', '–ß–∞—Å—Ç–∏—Ü–∞', '–î—Ä—É–≥–æ–µ'}
            
            smart_consolidated = {}
            
            _t_idx = _time.time()
            for base_lemma, pos_data in lemma_pos_analysis.items():
                # –û–±—â–∏–π —Å—á—ë—Ç—á–∏–∫ —Å–ª–æ–≤ –¥–ª—è –ª–µ–º–º—ã (–∏–º–µ–Ω–Ω–æ count, –∞ –Ω–µ POS)
                total_lemma_count = sum(data['total_count'] for data in pos_data.values())
                
                # –ò—â–µ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ —Å —Ä–æ–¥–æ–º (>33%)
                dominant_noun_key = None
                dominant_noun_count = 0
                target_noun_found = False
                
                if '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ' in pos_data:
                    noun_count = pos_data['–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ']['total_count']
                    if noun_count / total_lemma_count > 0.33:  # > 33% –æ—Ç –≤—Å–µ—Ö —Å–ª–æ–≤ –ª–µ–º–º—ã
                        # –ò—â–µ–º —Å–∞–º–æ–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –° –£–ö–ê–ó–ê–ù–ù–´–ú –†–û–î–û–ú
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ token_details –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–æ–¥–∞
                        noun_entries = pos_data['–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ']['entries']
                        
                        nouns_with_gender = {}
                        nouns_without_gender = {}
                        
                        for freq_key, count in noun_entries.items():
                            # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –≤ token_details –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è gender
                            gender = None
                            for safe_key, details in self.token_details.items():
                                if details['freq_key'] == freq_key:
                                    gender = safe_key[2]  # gender –∏–∑ (lemma, pos, gender)
                                    break
                            
                            if gender in ['Masc', 'Fem']:
                                nouns_with_gender[freq_key] = count
                            else:
                                nouns_without_gender[freq_key] = count
                        
                        # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º–æ–µ —á–∞—Å—Ç–æ—Ç–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ –° –†–û–î–û–ú (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
                        if nouns_with_gender:
                            dominant_noun_key = max(nouns_with_gender.keys(), key=lambda k: nouns_with_gender[k])
                            dominant_noun_count = noun_count
                            target_noun_found = True
                        # –ï—Å–ª–∏ –Ω–µ—Ç —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö —Å —Ä–æ–¥–æ–º, –Ω–æ –µ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –±–µ–∑ —Ä–æ–¥–∞
                        elif nouns_without_gender:
                            dominant_noun_key = max(nouns_without_gender.keys(), key=lambda k: nouns_without_gender[k])
                            dominant_noun_count = noun_count
                            target_noun_found = True
                            # –û–±–Ω—É–ª—è–µ–º nouns_with_gender, —Ç–∞–∫ –∫–∞–∫ –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –±–µ–∑ —Ä–æ–¥–∞
                            nouns_with_gender = {}
                
                if target_noun_found:
                    # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º: –¥–æ–±–∞–≤–ª—è–µ–º –º–µ–ª–∫–∏–µ POS –∫ —Ü–µ–ª–µ–≤–æ–º—É —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º—É
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π count —Ü–µ–ª–µ–≤–æ–≥–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ
                    if nouns_with_gender and dominant_noun_key in nouns_with_gender:
                        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –° –†–û–î–û–ú
                        consolidated_count = nouns_with_gender[dominant_noun_key]
                        consolidating_with_gender = True
                    else:
                        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º —Å —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –ë–ï–ó –†–û–î–ê
                        consolidated_count = nouns_without_gender[dominant_noun_key]
                        consolidating_with_gender = False
                    
                    minor_count = 0
                    
                    # 1. –î–æ–±–∞–≤–ª—è–µ–º –º–µ–ª–∫–∏–µ POS
                    for pos_ru, data in pos_data.items():
                        if pos_ru in minor_pos_types:
                            minor_count += data['total_count']
                            consolidated_count += data['total_count']
                    
                    # 2. –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω–æ–≥–æ —Ç–∏–ø–∞ (—Å —Ä–æ–¥–æ–º –∫ –±–µ–∑ —Ä–æ–¥–∞ –∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç)
                    if consolidating_with_gender:
                        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –° –†–û–î–û–ú: –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ë–ï–ó –†–û–î–ê
                        for freq_key, count in nouns_without_gender.items():
                            minor_count += count
                            consolidated_count += count
                    else:
                        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –ë–ï–ó –†–û–î–ê: –¥–æ–±–∞–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –° –†–û–î–û–ú (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        for freq_key, count in nouns_with_gender.items():
                            minor_count += count
                            consolidated_count += count
                    
                    # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                    # if minor_count > 0:
                    #     target_type = "—Å —Ä–æ–¥–æ–º" if consolidating_with_gender else "–±–µ–∑ —Ä–æ–¥–∞"
                    #     initial_count = nouns_with_gender[dominant_noun_key] if consolidating_with_gender else nouns_without_gender[dominant_noun_key]
                    #     print(f"üß† –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ª–µ–º–º—ã '{lemma}' ({target_type}): {initial_count} + {minor_count} = {consolidated_count}")
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    smart_consolidated[dominant_noun_key] = consolidated_count
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ (–∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤–æ–≥–æ –∏ —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö)
                    if consolidating_with_gender:
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –° –†–û–î–û–ú (–∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤–æ–≥–æ)
                        for freq_key, count in nouns_with_gender.items():
                            if freq_key != dominant_noun_key:
                                smart_consolidated[freq_key] = count
                        # nouns_without_gender —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Ü–µ–ª–µ–≤–æ–º—É
                    else:
                        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –ë–ï–ó –†–û–î–ê (–∫—Ä–æ–º–µ —Ü–µ–ª–µ–≤–æ–≥–æ)
                        for freq_key, count in nouns_without_gender.items():
                            if freq_key != dominant_noun_key:
                                smart_consolidated[freq_key] = count
                        # nouns_with_gender —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –∫ —Ü–µ–ª–µ–≤–æ–º—É
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ POS (–Ω–µ –º–µ–ª–∫–∏–µ –∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ)
                    for pos_ru, data in pos_data.items():
                        if pos_ru not in minor_pos_types and pos_ru != '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ':
                            for freq_key, freq in data['entries'].items():
                                smart_consolidated[freq_key] = freq
                else:
                    # –ù–µ—Ç –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–≥–æ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å —Ä–æ–¥–æ–º - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    for pos_ru, data in pos_data.items():
                        for freq_key, freq in data['entries'].items():
                            smart_consolidated[freq_key] = freq
            logger.debug(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ª–µ–º–º –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (dt={_time.time()-_t0:.2f}s)")
            
            # === –ö–û–ù–°–û–õ–ò–î–ê–¶–ò–Ø –í–ê–†–ò–ê–ù–¢–û–í –°–£–©–ï–°–¢–í–ò–¢–ï–õ–¨–ù–´–• (–ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è) ===
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ token_details –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–æ –ª–µ–º–º–µ + –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—é gender
            lemma_variants = {}  # lemma -> {'with_gender': {freq_key: freq}, 'without_gender': {freq_key: freq}}
            non_nouns = {}  # –ù–µ-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –∫–∞–∫ –µ—Å—Ç—å
            
            for freq_key, freq in smart_consolidated.items():
                # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å –≤ token_details
                details = None
                for safe_key, d in self.token_details.items():
                    if d['freq_key'] == freq_key:
                        details = d
                        lemma, pos_tag, gender = safe_key
                        break
                
                if details is None:
                    # –ó–∞–ø–∏—Å—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–º–æ–∂–µ—Ç –±—ã—Ç—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç)
                    non_nouns[freq_key] = freq
                    continue
                
                pos_ru = details['pos_ru']
                
                if pos_ru == '–°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ':
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ —Ä–µ–∞–ª—å–Ω–æ–π –ª–µ–º–º–µ –∏–∑ spaCy
                    if lemma not in lemma_variants:
                        lemma_variants[lemma] = {'with_gender': {}, 'without_gender': {}}
                    
                    if gender in ['Masc', 'Fem']:
                        lemma_variants[lemma]['with_gender'][freq_key] = freq
                    else:
                        lemma_variants[lemma]['without_gender'][freq_key] = freq
                else:
                    # –ù–µ-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    non_nouns[freq_key] = freq
            
            # –®–∞–≥ 2: –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö (–Ω–∞ –æ—Å–Ω–æ–≤–µ –†–ï–ê–õ–¨–ù–û–ì–û gender)
            consolidated_frequencies = {}
            
            for lemma, variants in lemma_variants.items():
                with_gender = variants['with_gender']
                without_gender = variants['without_gender']
                
                if with_gender:
                    # –ï—Å—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —Ä–æ–¥–æ–º - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –∏ —Å—É–º–º–∏—Ä—É–µ–º —Å –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –±–µ–∑ —Ä–æ–¥–∞
                    for freq_key, freq in with_gender.items():
                        if freq_key not in consolidated_frequencies:
                            consolidated_frequencies[freq_key] = 0
                        consolidated_frequencies[freq_key] += freq
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–æ—Ç—ã –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –±–µ–∑ —Ä–æ–¥–∞ –∫ –ø–æ–¥—Ö–æ–¥—è—â–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º —Å —Ä–æ–¥–æ–º
                    for freq_key_without, freq_without in without_gender.items():
                        # –ù–∞—Ö–æ–¥–∏–º –≤–∞—Ä–∏–∞–Ω—Ç —Å —Ç–µ–º –∂–µ —Ä–æ–¥–æ–º (–µ—Å–ª–∏ –º–æ–∂–Ω–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å)
                        best_match = None
                        best_freq = 0
                        
                        for freq_key_with in with_gender.keys():
                            if with_gender[freq_key_with] > best_freq:
                                best_match = freq_key_with
                                best_freq = with_gender[freq_key_with]
                        
                        if best_match:
                            consolidated_frequencies[best_match] += freq_without
                        else:
                            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π, –æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ —Ä–æ–¥–∞
                            consolidated_frequencies[freq_key_without] = freq_without
                else:
                    # –¢–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç—ã –±–µ–∑ —Ä–æ–¥–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    for freq_key, freq in without_gender.items():
                        consolidated_frequencies[freq_key] = freq
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            consolidated_frequencies.update(non_nouns)
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
            sorted_frequencies = sorted(consolidated_frequencies.items(), key=lambda x: x[1], reverse=True)
            
            # –°–æ–∑–¥–∞—ë–º DataFrame —Ç–æ–ª—å–∫–æ —Å –ù–û–í–´–ú–ò (–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏) —Å–ª–æ–≤–∞–º–∏
            data = []
            for word_with_pos, freq in sorted_frequencies:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–æ –∏ —á–∞—Å—Ç—å —Ä–µ—á–∏ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ "—Å–ª–æ–≤–æ (—á–∞—Å—Ç—å_—Ä–µ—á–∏)" –∏–ª–∏ "el —Å–ª–æ–≤–æ (—á–∞—Å—Ç—å_—Ä–µ—á–∏)"
                if ' (' in word_with_pos and word_with_pos.endswith(')'):
                    word_part = word_with_pos.split(' (')[0]  # "el capital" –∏–ª–∏ "capital"
                    pos_tag = word_with_pos.split(' (')[1].rstrip(')')
                else:
                    word_part = word_with_pos
                    pos_tag = '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
                
                # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –ª–µ–º–º—É –∏–∑ token_details (–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è spaCy)
                base_lemma = word_part  # Fallback
                for safe_key, details in self.token_details.items():
                    if details['freq_key'] == word_with_pos:
                        base_lemma = safe_key[0]  # lemma –∏–∑ (lemma, pos, gender)
                        break
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å–ª–æ–≤–æ –ù–ï –∏–∑–≤–µ—Å—Ç–Ω–æ –≤ Anki
                is_known = False
                comment = ""
                
                if self.word_comparator:
                    # –ü–æ–ø—Ä–æ–±—É–µ–º lemma-aware –ø—Ä–æ–≤–µ—Ä–∫—É, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                    if config.is_lemma_aware_known_enabled():
                        relevant_token_info = None
                        # –ò—â–µ–º —Ç–æ—á–Ω—ã–π freq_key (–Ω–∞–¥—ë–∂–Ω–µ–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ POS/–≥–µ–Ω–¥–µ—Ä–∞)
                        for safe_key, details in self.token_details.items():
                            if details.get('freq_key') == word_with_pos:
                                relevant_token_info = details
                                break
                        if not relevant_token_info:
                            # –§–æ–ª–±—ç–∫: –∏—â–µ–º –ø–æ –ª–µ–º–º–µ
                            for safe_key, details in self.token_details.items():
                                if safe_key[0] == base_lemma:
                                    relevant_token_info = details
                                    break
                        pos_code = relevant_token_info.get('pos', None) if relevant_token_info else None
                        gender_code = relevant_token_info.get('gender', None) if relevant_token_info else None
                        is_known = self.word_comparator.is_token_known(
                            lemma=base_lemma,
                            pos=pos_code,
                            gender=gender_code
                        )
                    else:
                        # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∫–∞–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è
                        is_known = self.word_comparator.is_word_known(word_part)
                    
                    if not is_known:
                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫–∏ –æ –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤–∞—Ö
                        relevant_token_info = None
                        for safe_key, details in self.token_details.items():
                            if safe_key[0] == base_lemma:  # lemma —Å–æ–≤–ø–∞–¥–∞–µ—Ç
                                relevant_token_info = details
                                break
                        similar = self.word_comparator.get_similar_candidates(
                            lemma=base_lemma,
                            pos=relevant_token_info.get('pos', 'UNKNOWN') if relevant_token_info else 'UNKNOWN',
                            gender=relevant_token_info.get('gender') if relevant_token_info else None
                        )
                        if similar:
                            comment = "–ü–æ—Ö–æ–∂–∏–µ –≤ ANKI: " + ", ".join(similar)
                        else:
                            comment = "–ù–æ–≤–æ–µ —Å–ª–æ–≤–æ"
                else:
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
                    is_known = self.is_word_known(word_part) or self.is_word_known(base_lemma)
                    comment = "ANKI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω" if not is_known else ""
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω–æ–π
                if not is_known and len(base_lemma) >= self.min_word_length:
                    # –í—ã—á–∏—Å–ª—è–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é —á–∞—Å—Ç–æ—Ç—É (–ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤)
                    relative_frequency = (freq / total_words) * 100 if total_words > 0 else 0
                    
                    # === –ü–†–ê–í–ò–õ–¨–ù–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï GENDER –ò–ó WORD (–ø–æ –ø—Ä–∞–≤–∏–ª—É) ===
                    # –í—ã—á–∏—Å–ª—è–µ–º Gender –∏–∑ –∏—Ç–æ–≥–æ–≤–æ–≥–æ Word, –∞ –Ω–µ –∏–∑ token_details
                    if word_part.startswith(("el ", "los ")):
                        gender = "Masc"
                    elif word_part.startswith(("la ", "las ")):
                        gender = "Fem"
                    else:
                        gender = "-"
                    
                    row = {
                        'Word': word_part,  # –° –∞—Ä—Ç–∏–∫–ª–µ–º –¥–ª—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
                        'Lemma': base_lemma,  # –ë–∞–∑–æ–≤–∞—è –ª–µ–º–º–∞ –±–µ–∑ –∞—Ä—Ç–∏–∫–ª—è
                        'Part of Speech': pos_tag,
                        'Gender': gender,  # –ë–µ—Ä—ë–º –∏–∑ word_part, –Ω–µ –∏–∑ token_details
                        'Frequency': f"{relative_frequency:.{decimal_places}f}%",
                        'Count': freq,
                        'Comments': comment or '-'
                    }
                    data.append(row)
            
            df = pd.DataFrame(data)
            logger.debug(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel: —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω DataFrame (rows={len(df)})")
            
            # –°—Ö–ª–æ–ø—ã–≤–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É
            # —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º Count (–¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∫–∞–∫ –≤ exporter.py)
            try:
                before_count = len(df)
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–ª–æ–≤—É (asc), –∑–∞—Ç–µ–º –ø–æ Count (desc) –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                df = df.sort_values(['Word', 'Count'], ascending=[True, False], kind='stable')
                # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è –ø–µ—Ä–≤—É—é (—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Count)
                df = df.drop_duplicates(subset=['Word'], keep='first').reset_index(drop=True)
                after_count = len(df)
                if after_count < before_count:
                    logger.info(f"–°—Ö–ª–æ–ø–Ω—É—Ç–æ –ø–æ —Å–ª–æ–≤–∞–º: –±—ã–ª–æ {before_count}, –æ—Å—Ç–∞–ª–æ—Å—å {after_count}")
            except Exception as e:
                # –í —Å–ª—É—á–∞–µ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º —Å–æ —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ–º ‚Äî –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ö–ª–æ–ø–Ω—É—Ç—å –ø–æ —Å–ª–æ–≤–∞–º: {e}")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é Count –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            df = df.sort_values('Count', ascending=False).reset_index(drop=True)
            
            # –°–æ–∑–¥–∞—ë–º Excel writer —Å –æ–¥–Ω–∏–º –ª–∏—Å—Ç–æ–º
            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                # –¢–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –ª–∏—Å—Ç —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
                df.to_excel(writer, sheet_name=sheet_name, index=False)
            
            logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {file_path}")
            logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(data)} –Ω–æ–≤—ã—Ö —Å–ª–æ–≤ (–∏–∑ {len(self.word_frequencies)} –≤—Å–µ–≥–æ)")
            logger.debug(f"–≠–∫—Å–ø–æ—Ä—Ç –≤ Excel: –∑–∞–≤–µ—Ä—à—ë–Ω (dt={_time.time()-_t0:.2f}s)")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —ç–∫—Å–ø–æ—Ä—Ç–µ –≤ Excel: {e}")
    
    
    def reset(self) -> None:
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –≤—Å—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        self.word_frequencies.clear()
        self.word_categories.clear()
        self.word_pos_tags.clear()
        logger.info("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–≤ —Å–±—Ä–æ—à–µ–Ω–∞")
