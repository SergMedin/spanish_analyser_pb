"""
ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ "antigÃ¼edad".

Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼, Ð²Ñ‹ÑÐ²Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼:
1. ÐšÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ñ/Ð±ÐµÐ· Ð°Ñ€Ñ‚Ð¸ÐºÐ»Ñ
2. ÐšÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸Ñ PROPNâ†’NOUN Ð´Ð»Ñ "AntigÃ¼edad" Ð² Ð½Ð°Ñ‡Ð°Ð»Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ
3. Ð•Ð´Ð¸Ð½Ñ‹Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº POS Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð²
4. ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Gender Ð¸Ð· Word ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸
"""

import sys
import os
from pathlib import Path

# Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ¾Ñ€Ð½ÐµÐ²ÑƒÑŽ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð² Ð¿ÑƒÑ‚ÑŒ
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.spanish_analyser.word_analyzer import WordAnalyzer
from src.spanish_analyser.components.spacy_manager import SpacyManager
from src.spanish_analyser.components.pos_tagger import POSTagger


def test_antiguedad_fixes():
    """Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð½Ð° Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ antigÃ¼edad."""
    
    print("ðŸ”§ Ð¢ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð´Ð»Ñ 'antigÃ¼edad'")
    print("=" * 60)
    
    # Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°Ð¼Ð¸
    test_text = """
    AntigÃ¼edad es un concepto importante en arqueologÃ­a. 
    La antigÃ¼edad del objeto fue determinada por carbono-14.
    El estudio de la antigÃ¼edad clÃ¡sica es fascinante.
    Las antigÃ¼edades se venden en este mercado.
    """
    
    print(f"ðŸ“ Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚:\n{test_text.strip()}\n")
    
    # Ð¢ÐµÑÑ‚ 1: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° spaCy
    print("ðŸ” Ð¢ÐµÑÑ‚ 1: SpacyManager Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸ÑÐ¼Ð¸ POS")
    spacy_manager = SpacyManager()
    doc = spacy_manager.analyze_text_with_corrections(test_text)
    
    print("ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ†Ð¸ÑÐ¼Ð¸:")
    for token in doc:
        if token.is_alpha and len(token.text) >= 3:
            print(f"  {token.text} -> {token.pos_} (Ð»ÐµÐ¼Ð¼Ð°: {token.lemma_})")
    
    # Ð¢ÐµÑÑ‚ 2: Ð•Ð´Ð¸Ð½Ñ‹Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº POS Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð²
    print(f"\nðŸ—£ï¸ Ð¢ÐµÑÑ‚ 2: Ð•Ð´Ð¸Ð½Ñ‹Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº POS Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð²")
    pos_tagger = POSTagger()
    print("ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… POS:")
    for pos in ['NOUN', 'PROPN', 'ADJ', 'VERB', 'SYM']:
        print(f"  {pos} -> {pos_tagger.get_pos_tag_ru(pos)}")
    
    # Ð¢ÐµÑÑ‚ 3: WordAnalyzer Ñ Ð½Ð¾Ð²Ð¾Ð¹ Ð»Ð¾Ð³Ð¸ÐºÐ¾Ð¹
    print(f"\nðŸ“Š Ð¢ÐµÑÑ‚ 3: WordAnalyzer Ñ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹")
    analyzer = WordAnalyzer()
    analyzer.add_words_from_text(test_text)
    
    print("Ð§Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ ÑÐ»Ð¾Ð² Ð´Ð¾ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°:")
    for word_pos, freq in analyzer.word_frequencies.most_common():
        if 'antigÃ¼edad' in word_pos.lower():
            print(f"  {word_pos}: {freq}")
    
    # Ð¢ÐµÑÑ‚ 4: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð² ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ðµ
    print(f"\nðŸ—‚ï¸ Ð¢ÐµÑÑ‚ 4: Ð¡Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ñ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸ÐµÐ¹")
    
    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Excel Ñ„Ð°Ð¹Ð» Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸
    import tempfile
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
        analyzer.export_to_excel(tmp_file.name)
        
        # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
        import pandas as pd
        df = pd.read_excel(tmp_file.name)
        
        # Ð¤Ð¸Ð»ÑŒÑ‚Ñ€ÑƒÐµÐ¼ ÑÑ‚Ñ€Ð¾ÐºÐ¸ Ñ antigÃ¼edad
        antiguedad_rows = df[df['Lemma'].str.contains('antigÃ¼edad', case=False, na=False)]
        
        print("Ð¡Ñ‚Ñ€Ð¾ÐºÐ¸ Ñ 'antigÃ¼edad' Ð² Excel:")
        if not antiguedad_rows.empty:
            for _, row in antiguedad_rows.iterrows():
                print(f"  Word: '{row['Word']}' | POS: '{row['Part of Speech']}' | Gender: '{row['Gender']}' | Count: {row['Count']}")
        else:
            print("  âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ ÑÑ‚Ñ€Ð¾Ðº Ñ 'antigÃ¼edad'")
        
        # ÐžÑ‡Ð¸ÑÑ‚ÐºÐ°
        os.unlink(tmp_file.name)
    
    # Ð¢ÐµÑÑ‚ 5: ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº
    print(f"\nðŸ“ˆ Ð¢ÐµÑÑ‚ 5: ÐšÐ°Ñ‡ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸")
    quality_stats = spacy_manager.get_quality_statistics(doc)
    
    print(f"Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°:")
    print(f"  Ð’ÑÐµÐ³Ð¾ alpha Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {quality_stats['total_alpha_tokens']}")
    print(f"  X/SYM Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²: {quality_stats['x_sym_tokens']} ({quality_stats['x_sym_ratio']:.1%})")
    print(f"  Ð”Ð¾Ð»Ñ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… POS: {quality_stats['main_pos_ratio']:.1%}")
    
    if quality_stats['quality_warnings']:
        print("ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°:")
        for warning in quality_stats['quality_warnings']:
            print(f"  {warning}")
    else:
        print("  âœ… ÐŸÑ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´ÐµÐ½Ð¸Ð¹ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð½ÐµÑ‚")
    
    print("\n" + "=" * 60)
    print("âœ… Ð’ÑÐµ Ñ‚ÐµÑÑ‚Ñ‹ Ð¸ÑÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ñ‹!")
    print("ðŸ’¡ ÐžÐ¶Ð¸Ð´Ð°ÐµÐ¼Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:")
    print("  - 'AntigÃ¼edad' Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð¾ ÐºÐ°Ðº NOUN, Ð° Ð½Ðµ PROPN")
    print("  - Ð’Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ 'antigÃ¼edad' Ð¸ 'la antigÃ¼edad' Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹")
    print("  - Gender Ð´Ð¾Ð»Ð¶ÐµÐ½ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¾Ð²Ð°Ñ‚ÑŒ Ð°Ñ€Ñ‚Ð¸ÐºÐ»ÑŽ Ð² Word ÐºÐ¾Ð»Ð¾Ð½ÐºÐµ")
    print("  - Ð’ÑÐµ POS Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐµÐ´Ð¸Ð½Ð¾Ð¾Ð±Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸")


def test_specific_consolidation_logic():
    """Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð»Ð¾Ð³Ð¸ÐºÑƒ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð¼ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ðµ."""
    print(f"\nðŸ§ª Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹ Ñ‚ÐµÑÑ‚: Ð»Ð¾Ð³Ð¸ÐºÐ° ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸")
    print("-" * 40)
    
    # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ‚ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ ÐºÐ°Ðº ÐµÑÐ»Ð¸ Ð±Ñ‹ Ð¾Ð½Ð¸ Ð¿Ñ€Ð¸ÑˆÐ»Ð¸ Ð¸Ð· word_frequencies
    test_frequencies = {
        'antigÃ¼edad (Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ)': 2,      # Ð±ÐµÐ· Ð°Ñ€Ñ‚Ð¸ÐºÐ»Ñ
        'la antigÃ¼edad (Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ)': 11,  # Ñ Ð°Ñ€Ñ‚Ð¸ÐºÐ»ÐµÐ¼ Fem
        'capital (ÐŸÑ€Ð¸Ð»Ð°Ð³Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ)': 5,          # Ð½Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ - Ð½Ðµ Ñ‚Ñ€Ð¾Ð³Ð°ÐµÐ¼
    }
    
    print("Ð˜ÑÑ…Ð¾Ð´Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹:")
    for word_pos, freq in test_frequencies.items():
        print(f"  {word_pos}: {freq}")
    
    # Ð¡Ð¸Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐµÐ¼ Ð»Ð¾Ð³Ð¸ÐºÑƒ ÐºÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð¸Ð· export_to_excel
    lemma_variants = {}
    non_nouns = {}
    
    for word_with_pos, freq in test_frequencies.items():
        if ' (' in word_with_pos and word_with_pos.endswith(')'):
            word_part = word_with_pos.split(' (')[0]
            pos_tag = word_with_pos.split(' (')[1].rstrip(')')
        else:
            continue
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð»ÐµÐ¼Ð¼Ñƒ
        if word_part.startswith(('el ', 'la ')):
            base_lemma = word_part.split(' ', 1)[1]
            has_article = True
        else:
            base_lemma = word_part
            has_article = False
        
        if pos_tag == 'Ð¡ÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ':
            if base_lemma not in lemma_variants:
                lemma_variants[base_lemma] = {'with_article': {}, 'without_article': {}}
            
            if has_article:
                lemma_variants[base_lemma]['with_article'][word_with_pos] = freq
            else:
                lemma_variants[base_lemma]['without_article'][word_with_pos] = freq
        else:
            non_nouns[word_with_pos] = freq
    
    print(f"\nÐÐ½Ð°Ð»Ð¸Ð· Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð² ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ…:")
    for base_lemma, variants in lemma_variants.items():
        print(f"  Ð›ÐµÐ¼Ð¼Ð° '{base_lemma}':")
        print(f"    Ð¡ Ð°Ñ€Ñ‚Ð¸ÐºÐ»ÐµÐ¼: {variants['with_article']}")
        print(f"    Ð‘ÐµÐ· Ð°Ñ€Ñ‚Ð¸ÐºÐ»Ñ: {variants['without_article']}")
    
    # ÐšÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÐ¼
    consolidated_frequencies = {}
    
    for base_lemma, variants in lemma_variants.items():
        with_article = variants['with_article']
        without_article = variants['without_article']
        
        if with_article:
            # Ð•ÑÑ‚ÑŒ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ñ Ð°Ñ€Ñ‚Ð¸ÐºÐ»ÐµÐ¼ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ñ…
            for freq_key, freq in with_article.items():
                consolidated_frequencies[freq_key] = freq
            
            # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð¾Ð² Ð±ÐµÐ· Ð°Ñ€Ñ‚Ð¸ÐºÐ»Ñ Ðº Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ð°Ð¼ Ñ Ð°Ñ€Ñ‚Ð¸ÐºÐ»ÐµÐ¼  
            for freq_key_without, freq_without in without_article.items():
                for freq_key_with in with_article.keys():
                    word_with = freq_key_with.split(' (')[0]
                    if word_with.endswith(' ' + base_lemma):
                        consolidated_frequencies[freq_key_with] += freq_without
                        break
        else:
            # Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð°Ñ€Ð¸Ð°Ð½Ñ‚Ñ‹ Ð±ÐµÐ· Ð°Ñ€Ñ‚Ð¸ÐºÐ»Ñ
            for freq_key, freq in without_article.items():
                consolidated_frequencies[freq_key] = freq
    
    consolidated_frequencies.update(non_nouns)
    
    print(f"\nÐšÐ¾Ð½ÑÐ¾Ð»Ð¸Ð´Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹:")
    for word_pos, freq in consolidated_frequencies.items():
        print(f"  {word_pos}: {freq}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ð¾ÑÑ‚ÑŒ Gender
        word_part = word_pos.split(' (')[0]
        if word_part.startswith("el "):
            expected_gender = "Masc"
        elif word_part.startswith("la "):
            expected_gender = "Fem"
        else:
            expected_gender = "-"
        print(f"    -> Expected Gender: {expected_gender}")


if __name__ == "__main__":
    test_antiguedad_fixes()
    test_specific_consolidation_logic()
